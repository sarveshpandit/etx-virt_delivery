{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a14ab8ad-bfb0-4e84-9d0b-035ee66bdfef",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This Python 3 notebook explores the exported VMWare inventory generated by [RVTools](https://www.robware.net/) and generates various statistical analyses of the data for assessing the scale, complexity, and other characteristics.\n",
    "\n",
    "The analysis is directed towards understanding the feasibility of migrating these VMWare virtual machines to Red Hat's OpenShift Virtualization Platform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e010991-ca96-43de-af5f-cd22bfdbd72e",
   "metadata": {},
   "source": [
    "# Initial Setup\n",
    "\n",
    "This section configures the script and the required packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Required Packages\n",
    "\n",
    "This script uses `pandas` and `numpy` for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the VM inventory generated by rvtools\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "Set up a few options and other configuration parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the display option to prevent line wrapping\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.expand_frame_repr', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Set up the data directory\n",
    "First, set up the directory where the RVTools Excel data files will be stored. This folder must also contain an index.xlsx file (refer to the provided template for the expected format). The index.xlsx file should list the valid RVTools Excel file names along with their corresponding vCenter instances. This script assumes that there is one Excel file per vCenter instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current script directory\n",
    "current_dir = \".\"\n",
    "\n",
    "# Specify the directory containing the Excel files\n",
    "DATA_DIR = os.path.join(current_dir, '../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and configuration parameters\n",
    "\n",
    "# The index file\n",
    "INDEX_FILENAME = \"index.xlsx\"\n",
    "INDEX_FILEPATH = os.path.join(DATA_DIR, INDEX_FILENAME)\n",
    "INDEX_SHEETNAME = \"index\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Functions\n",
    "\n",
    "This section defines a few functions for decomposing the analysis code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_rvtools_excel_files(directory, filenames_to_process):\n",
    "    \"\"\"\n",
    "    Reads multiple Excel files exported from RVtools software and returns a dictionary\n",
    "    with filenames as keys and a dictionary of DataFrames (one for each sheet) as values.\n",
    "\n",
    "    The `index.xlsx` and `index_template.xlsx` files are explicitly ignored.\n",
    "\n",
    "    Parameters:\n",
    "    directory (str): The directory containing the Excel files.\n",
    "\n",
    "    filenames_to_process (list): A list of filenames to process.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the DataFrames from each Excel file.\n",
    "    \"\"\"\n",
    "    rvtools_data = {}\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        # Only process files listed in the index file.\n",
    "        filename_base, _ = os.path.splitext(filename)\n",
    "        if filename_base not in filenames_to_process: continue\n",
    "        if filename in ['index.xlsx', 'index_template.xlsx']: continue\n",
    "\n",
    "        if filename.endswith('.xlsx') or filename.endswith('.xls'):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            excel_data = pd.read_excel(filepath, sheet_name=None)\n",
    "            rvtools_data[filename] = excel_data\n",
    "\n",
    "    return rvtools_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "# Read, Clean and Filter the RVTools data\n",
    "\n",
    "This section reads the `RVTools` exported files. It uses an `index.xlsx` metadata file to identify the\n",
    "in-scope `vCenter` instances for the migration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Read the index metadata file\n",
    "\n",
    "First read the index file to determine the RVTools files to process. This file contains additional metadata,\n",
    "including the `vCenter` instances in scope, which can be customized to process specific instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First read the index Excel file\n",
    "import pandas as pd\n",
    "\n",
    "# Read the index Excel file\n",
    "index_df = pd.read_excel(\n",
    "    INDEX_FILEPATH, sheet_name=INDEX_SHEETNAME, \n",
    "    nrows=19, index_col='vCenter', \n",
    "    true_values=['Yes', 'Y'], false_values=['No', 'N'], \n",
    "    na_filter=False, dtype={'In Scope': bool}\n",
    ")\n",
    "\n",
    "# Clean up column names and handle missing values\n",
    "index_df.columns = index_df.columns.str.replace(' ', '_')\n",
    "index_df.fillna('', inplace=True)\n",
    "\n",
    "# Filter for in-scope vCenters\n",
    "inscope_df = index_df[index_df['In_Scope']].reset_index()\n",
    "\n",
    "# Count occurrences of each vCenter\n",
    "pivot_table = inscope_df.groupby('vCenter').size().reset_index(name='Count')\n",
    "\n",
    "# Print in a structured table format\n",
    "print(\"\\nIn-Scope vCenter Instances:\")\n",
    "print(pivot_table.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Read the RVTool Exported Spreadsheets\n",
    "\n",
    "Read all the RVTools exported files available in the `data` directory. The data will be read into a dictionary,\n",
    "with one entry per file, where the key is the filename, and the value is **another** nested dictionary with the spreadsheet's\n",
    "sheet name as the key, and a `DataFrame` containing the sheets values.\n",
    "\n",
    "The files **need** to be in the Microsoft `xlsx` format.\n",
    "\n",
    "**Note**: This step will take a few minutes to complete. Be patient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Read the index Excel file\n",
    "index_df = pd.read_excel(\n",
    "    INDEX_FILEPATH, sheet_name=INDEX_SHEETNAME, \n",
    "    nrows=19, index_col='vCenter', \n",
    "    true_values=['Yes', 'Y'], false_values=['No', 'N'], \n",
    "    na_filter=False, dtype={'In Scope': bool}\n",
    ")\n",
    "\n",
    "# Clean up column names and handle missing values\n",
    "index_df.columns = index_df.columns.str.replace(' ', '_')\n",
    "index_df.fillna('', inplace=True)\n",
    "\n",
    "# Ensure 'In_Scope' column is boolean\n",
    "index_df['In_Scope'] = index_df['In_Scope'].astype(bool)\n",
    "\n",
    "# Filter for in-scope vCenters\n",
    "inscope_df = index_df[index_df['In_Scope']].reset_index()\n",
    "\n",
    "# Extract list of in-scope vCenter instances\n",
    "inscope_vcenter_instances = inscope_df['vCenter'].tolist()\n",
    "\n",
    "# Function to read RVTools Excel files while excluding 'index_template.xlsx' and 'index.xlsx'\n",
    "def read_rvtools_excel_files(directory, vcenters):\n",
    "    rvtools_data = {}\n",
    "    exclude_files = {\"index_template.xlsx\", \"index.xlsx\"}  # Set of filenames to exclude\n",
    "\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".xlsx\") and file.lower() not in exclude_files:\n",
    "            file_path = os.path.join(directory, file)\n",
    "            try:\n",
    "                xls = pd.ExcelFile(file_path)\n",
    "                sheets = {sheet: xls.parse(sheet) for sheet in xls.sheet_names}\n",
    "                rvtools_data[file] = sheets\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file}: {e}\")\n",
    "    \n",
    "    return rvtools_data\n",
    "\n",
    "# Read the RVTools Excel files\n",
    "rvtools_data = read_rvtools_excel_files(\"../data\", inscope_vcenter_instances)\n",
    "\n",
    "# Display the loaded data (for demonstration purposes)\n",
    "for filename, sheets in rvtools_data.items():\n",
    "    print(f'Processed RVTools File: {filename}')\n",
    "    for sheet_name, df in sheets.items():\n",
    "        print(f\"  Sheet: {sheet_name} Shape: {df.shape}\")\n",
    "\n",
    "print(f'Total files processed: {len(rvtools_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Create the consolidated data frames\n",
    "\n",
    "Create two consolidated dataframes containing information from all `vCenter` instances:\n",
    "\n",
    "1. One dataframe containing the `vInfo` details\n",
    "2. The second one containing the `vHost` details\n",
    "\n",
    "These will be used later to summarize the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming rvtools_data is already populated with the sheets data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the lists\n",
    "vinfo_sheets = []\n",
    "vhost_sheets = []\n",
    "\n",
    "# Load vInfo and vHost sheets into the lists\n",
    "for filename, sheets in rvtools_data.items():\n",
    "    vCenter = filename.split('.')[0].lower()  # Extract vCenter instance name from the filename\n",
    "\n",
    "    # Ensure 'vInfo' and 'vHost' sheets exist before processing\n",
    "    if 'vInfo' in sheets and 'vHost' in sheets:\n",
    "        # Add the vCenter instance name to each sheet\n",
    "        sheets['vInfo']['vCenter'] = vCenter\n",
    "        sheets['vHost']['vCenter'] = vCenter\n",
    "\n",
    "        # Append to the respective lists\n",
    "        vinfo_sheets.append(sheets['vInfo'])\n",
    "        vhost_sheets.append(sheets['vHost'])\n",
    "\n",
    "# Filter out empty or all-NA DataFrames\n",
    "vinfo_sheets = [df for df in vinfo_sheets if not df.dropna(how='all').empty]\n",
    "vhost_sheets = [df for df in vhost_sheets if not df.dropna(how='all').empty]\n",
    "\n",
    "# Concatenate the filtered DataFrames only if there are valid sheets\n",
    "consolidated_vinfo_df = pd.concat(vinfo_sheets, ignore_index=True) if vinfo_sheets else pd.DataFrame()\n",
    "consolidated_vhost_df = pd.concat(vhost_sheets, ignore_index=True) if vhost_sheets else pd.DataFrame()\n",
    "\n",
    "# Verify data ingestion success with improved readability\n",
    "print(\"\\nâœ… Data Ingestion Verification âœ…\")\n",
    "print(f\"Total vInfo records: {len(consolidated_vinfo_df)}\")\n",
    "print(f\"Total vHost records: {len(consolidated_vhost_df)}\\n\")\n",
    "\n",
    "if not consolidated_vinfo_df.empty:\n",
    "    print(\"ðŸ”¹ Sample vInfo Data:\")\n",
    "    print(consolidated_vinfo_df.head().to_string(index=False), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726fe759-4222-4227-a0e1-896b7e60de1b",
   "metadata": {},
   "source": [
    "### Distribution of VMs per In-Scope vCenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Group by vCenter and count VMs\n",
    "grouped_summary = consolidated_vinfo_df.groupby(\"vCenter\").size().reset_index(name=\"Count\")\n",
    "\n",
    "# Print summary\n",
    "total_vms = grouped_summary[\"Count\"].sum()\n",
    "total_vcenters = grouped_summary.shape[0]\n",
    "\n",
    "print(f\"Overall Distribution of {total_vms:,} VMs in the {total_vcenters:,} vCenter instances:\\n\")\n",
    "print(grouped_summary)\n",
    "print(\"\\nNote: This is the OVERALL VM count and may include VM templates and SRM placeholders\")\n",
    "\n",
    "# Plot a pie chart\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(\n",
    "    grouped_summary[\"Count\"],\n",
    "    labels=grouped_summary[\"vCenter\"],\n",
    "    autopct=\"%1.1f%%\",\n",
    "    startangle=140,\n",
    ")\n",
    "plt.title(\"Distribution of ALL VMs by vCenter Instances\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Clean the VM List\n",
    "Filter the consolidated `vInfo` content using the following criteria:\n",
    "\n",
    "1. Remove all `template` entries\n",
    "2. Remove all `SRM Placeholder` entries\n",
    "3. Remove all `orphaned VM object` entries\n",
    "4. Remove all `other objects`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61863a1-bd86-445c-b261-d22d12c13cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Define patterns to ignore in the VM column\n",
    "ignore_patterns = [\"virtual_appliance\", \"virtual appliance\", \"CTX\"]\n",
    "\n",
    "# Define OS types to filter out\n",
    "os_filter_patterns = [\n",
    "    \"Microsoft Windows 10 (64-bit)\", \"AlmaLinux (64-bit)\", \"CentOS 4/5 (64-bit)\", \"panos\",\n",
    "    \"CentOS 4/5/6/7 (64bit-)\", \"CentOS 7 (64-bit)\", \"CentOS 8 (64-bit)\", \"Debian GNU/Linux 11 (64-bit)\",\n",
    "    \"Other (32-bit)\", \"Other (64-bit)\", \"Other 3.x Linux (64-bit)\", \"Other 3.x or later Linux (64-bit)\",\n",
    "    \"Other Linux (64-bit)\", \"SUSE Linux Enterprise 11 (64-bit)\", \"SUSE Linux Enterprise 12 (64-bit)\",\n",
    "    \"SUSE Linux Enterprise 15 (64-bit)\", \"Ubuntu Linux (64-bit)\", \"VMware Photon OS (64-bit)\",\n",
    "]\n",
    "\n",
    "# Normalize the VM column for consistent matching\n",
    "if 'VM' in consolidated_vinfo_df.columns:\n",
    "    consolidated_vinfo_df['VM'] = consolidated_vinfo_df['VM'].astype(str).str.strip().str.lower()\n",
    "\n",
    "# Normalize ignore patterns for VM filtering\n",
    "ignore_patterns_lower = [pattern.lower() for pattern in ignore_patterns]\n",
    "regex_vm_ignore = '|'.join([re.escape(pattern) for pattern in ignore_patterns_lower])\n",
    "\n",
    "# Normalize OS filter patterns for OS filtering\n",
    "os_filter_patterns_lower = [os_type.lower() for os_type in os_filter_patterns]\n",
    "\n",
    "# Check if 'Total disk capacity MiB' exists, if not, default to 1 (to prevent exclusion)\n",
    "if 'Total disk capacity MiB' in consolidated_vinfo_df.columns:\n",
    "    consolidated_vinfo_df['Total disk capacity MiB'] = consolidated_vinfo_df['Total disk capacity MiB'].fillna(0)\n",
    "else:\n",
    "    print(\"âš ï¸ Warning: 'Total disk capacity MiB' column is missing! Defaulting to non-zero values.\")\n",
    "    consolidated_vinfo_df['Total disk capacity MiB'] = 1  # Prevents exclusion\n",
    "\n",
    "# Define filter conditions\n",
    "filter_condition = (\n",
    "    # Ensure VM does not match ignore patterns\n",
    "    (~consolidated_vinfo_df.get('VM', pd.Series('')).str.contains(regex_vm_ignore, case=False, na=False)) &\n",
    "    # Ensure OS columns do not match os_filter_patterns\n",
    "    (~consolidated_vinfo_df.get('OS according to the VMware Tools', pd.Series('')).str.lower().isin(os_filter_patterns_lower)) &\n",
    "    (~consolidated_vinfo_df.get('OS according to the configuration file', pd.Series('')).str.lower().isin(os_filter_patterns_lower)) &\n",
    "    # Exclude templates\n",
    "    (consolidated_vinfo_df.get('Template', pd.Series(False)) == False) &\n",
    "    # Exclude SRM placeholders\n",
    "    (consolidated_vinfo_df.get('SRM Placeholder', pd.Series(False)) == False) &\n",
    "    # Exclude orphaned VMs\n",
    "    (consolidated_vinfo_df.get('Connection state', pd.Series('')) != 'orphaned') &\n",
    "    # Exclude powered-off VMs\n",
    "    (consolidated_vinfo_df.get('Powerstate', pd.Series('')) != 'poweredOff') &\n",
    "    # Exclude VMs with zero disk capacity\n",
    "    (consolidated_vinfo_df['Total disk capacity MiB'] != 0)\n",
    ")\n",
    "\n",
    "# Apply filter to get \"in-scope\" VMs\n",
    "filtered_vinfo_df = consolidated_vinfo_df[filter_condition]\n",
    "ignored_vm_artifacts = len(consolidated_vinfo_df) - len(filtered_vinfo_df)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nâœ… **Filtering Summary** âœ…\")\n",
    "print(f\"ðŸ”¹ Removed: {ignored_vm_artifacts:,} templates, SRM placeholders, orphaned, powered-off VMs, and excluded patterns.\")\n",
    "print(f\"ðŸ”¹ Filtered (In-Scope) VM count: {len(filtered_vinfo_df):,}.\\n\")\n",
    "\n",
    "# Display the list of \"in-scope\" VMs after filtering\n",
    "if not filtered_vinfo_df.empty:\n",
    "    print(\"\\nðŸ” In-Scope of Filtered VMs:\")\n",
    "    display(filtered_vinfo_df[['VM', 'OS according to the VMware Tools', 'OS according to the configuration file']].head())\n",
    "\n",
    "# Debug: Verify that ignore patterns and OS filters were respected\n",
    "print(\"\\nðŸ” Out-of-Scope Filtered VMs\")\n",
    "print(filtered_vinfo_df[['VM', 'OS according to the VMware Tools', 'OS according to the configuration file']].drop_duplicates().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Further filter down the In-Scope List\n",
    "Filter down to the in-scope subset of the `vCenter` instances for further analysis.\n",
    "\n",
    "This selection of the scoped instances is provided by the `index.xlsx` metadata file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Select the in-scope vCenter instances\n",
    "inscope_vinfo_condition = filtered_vinfo_df['vCenter'].isin(inscope_vcenter_instances)\n",
    "inscope_vinfo_df = filtered_vinfo_df[inscope_vinfo_condition]\n",
    "\n",
    "# Summary stats for in-scope VMs\n",
    "inscope_vm_count = len(inscope_vinfo_df)\n",
    "percent_inscope_vms = (inscope_vm_count / len(filtered_vinfo_df)) * 100.0\n",
    "\n",
    "print(\"\\nâœ… VM Scope Summary\")\n",
    "print(f\"ðŸ”¹ {inscope_vm_count:,} VMs are in-scope ({percent_inscope_vms:0.2f}% of {len(filtered_vinfo_df):,} total VMs).\\n\")\n",
    "\n",
    "# Create a pivot table for VMs by vCenter\n",
    "vm_pivot = inscope_vinfo_df.pivot_table(index=\"vCenter\", aggfunc=\"size\").reset_index()\n",
    "vm_pivot.columns = [\"vCenter\", \"VM Count\"]\n",
    "display(vm_pivot)\n",
    "\n",
    "# Select the in-scope hosts\n",
    "inscope_vhost_condition = consolidated_vhost_df['vCenter'].isin(inscope_vcenter_instances)\n",
    "inscope_vhost_df = consolidated_vhost_df[inscope_vhost_condition]\n",
    "\n",
    "# Summary stats for in-scope hosts\n",
    "inscope_host_count = len(inscope_vhost_df)\n",
    "percent_inscope_hosts = (inscope_host_count / len(consolidated_vhost_df)) * 100.0\n",
    "\n",
    "print(\"\\nâœ… Host Scope Summary\")\n",
    "print(f\"ðŸ”¹ {inscope_host_count:,} hosts are in-scope ({percent_inscope_hosts:0.2f}% of {len(consolidated_vhost_df):,} total hosts).\\n\")\n",
    "\n",
    "# Create a pivot table for hosts by vCenter\n",
    "host_pivot = inscope_vhost_df.pivot_table(index=\"vCenter\", aggfunc=\"size\").reset_index()\n",
    "host_pivot.columns = [\"vCenter\", \"Host Count\"]\n",
    "display(host_pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "# Perform the Analysis\n",
    "\n",
    "The primary analysis begins from this section. We compute the following:\n",
    "\n",
    "1. A consolidated summary of _all_ `vCenter` instances (including the out of scope instances)\n",
    "2. A count of the **in-scope** guest operating systems\n",
    "3. A grouping of the disk usage by tiers, for the **in-scope** VMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### 1- Create a consolidated view of the VM landscape\n",
    "\n",
    "This is a pivot table centered on the vCenter instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pivot table for the VM info\n",
    "# Create a pivot table for the VM info with filtered in-scope VMs\n",
    "vinfo_pivot_df = filtered_vinfo_df.pivot_table(\n",
    "    index='vCenter',\n",
    "    values=['VM', 'CPUs', 'Memory', 'NICs', 'Total disk capacity MiB'],\n",
    "    aggfunc={\n",
    "        'VM': 'count',\n",
    "        'CPUs': 'sum',\n",
    "        'Memory': 'sum',\n",
    "        'NICs': 'sum',\n",
    "        'Total disk capacity MiB': 'sum'\n",
    "    },\n",
    "    margins=False\n",
    ")\n",
    "\n",
    "# Create a pivot table for in-scope VM count only\n",
    "in_scope_vm_count_df = filtered_vinfo_df.pivot_table(\n",
    "    index='vCenter',\n",
    "    values='VM',\n",
    "    aggfunc='count'\n",
    ").rename(columns={'VM': 'In-Scope VM Count'})\n",
    "\n",
    "# Host pivot table for additional host info\n",
    "vhost_pivot_df = consolidated_vhost_df.pivot_table(\n",
    "    index='vCenter',\n",
    "    values=['Host', '# VMs total', '# CPU', '# Cores'],\n",
    "    aggfunc={\n",
    "        'Host': 'count',\n",
    "        '# VMs total': 'sum',\n",
    "        '# CPU': 'sum',\n",
    "        '# Cores': 'sum'\n",
    "    },\n",
    "    margins=False\n",
    ")\n",
    "\n",
    "# Join tables and include the in-scope VM count\n",
    "consolidated_summary_df = vinfo_pivot_df.join(vhost_pivot_df).join(in_scope_vm_count_df, how='left')\n",
    "consolidated_summary_df['In-Scope VM Count'] = consolidated_summary_df['In-Scope VM Count'].fillna(0).astype(int)\n",
    "\n",
    "# Calculate column totals\n",
    "consolidated_summary_totals = consolidated_summary_df.sum(axis=0).to_frame().T\n",
    "consolidated_summary_totals.index = ['Total']\n",
    "\n",
    "# Append totals row to the consolidated summary\n",
    "consolidated_summary_with_totals = pd.concat([consolidated_summary_df, consolidated_summary_totals])\n",
    "consolidated_summary_with_totals.index.name = 'vCenter'\n",
    "\n",
    "# Display the final summary with in-scope VM count\n",
    "consolidated_summary_with_totals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### 2- Summarize the Operating Systems\n",
    "\n",
    "In this section, we summarize the guest operating systems for the **in-scope** `vCenter` instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the Operating Systems and list by count.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create a combined OS column\n",
    "inscope_vinfo_df['Final OS'] = np.where(\n",
    "    inscope_vinfo_df['OS according to the VMware Tools'].notnull() & \n",
    "    (inscope_vinfo_df['OS according to the VMware Tools'].str.strip() != ''),\n",
    "    inscope_vinfo_df['OS according to the VMware Tools'].str.strip(),\n",
    "    inscope_vinfo_df['OS according to the configuration file'].str.strip()\n",
    ")\n",
    "\n",
    "# Ensure the 'VM' column exists and has valid data\n",
    "if 'VM' not in inscope_vinfo_df.columns:\n",
    "    raise KeyError(\"'VM' column is missing from the DataFrame.\")\n",
    "\n",
    "# Create the pivot table and sort by VM count in descending order\n",
    "inscope_guest_os_pivot = inscope_vinfo_df.pivot_table(index='Final OS', values='VM', aggfunc='count')\n",
    "inscope_guest_os_pivot = inscope_guest_os_pivot.sort_values(by='VM', ascending=True)  # Ascending for bar chart\n",
    "\n",
    "# Check if pivot table is empty\n",
    "if inscope_guest_os_pivot.empty:\n",
    "    raise ValueError(\"The pivot table is empty. Check your input data.\")\n",
    "\n",
    "# Calculate percentages\n",
    "total_vms = inscope_guest_os_pivot['VM'].sum()\n",
    "percentages = (inscope_guest_os_pivot['VM'] / total_vms * 100).round(1)\n",
    "\n",
    "# Create figure and axis\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Create horizontal bar chart\n",
    "bars = ax.barh(inscope_guest_os_pivot.index, inscope_guest_os_pivot['VM'], color=plt.cm.tab10.colors)\n",
    "\n",
    "# Add labels to bars\n",
    "for bar, count, percentage in zip(bars, inscope_guest_os_pivot['VM'], percentages):\n",
    "    ax.text(bar.get_width() + 2, bar.get_y() + bar.get_height()/2, \n",
    "            f\"{count} VMs ({percentage}%)\", va='center', fontsize=8, color='black')\n",
    "\n",
    "# Style and labels\n",
    "ax.set_xlabel(\"Number of VMs\", fontsize=10)\n",
    "ax.set_ylabel(\"Operating Systems\", fontsize=10)\n",
    "ax.set_title(\"Guest OS Distribution for In-Scope VMs\", fontsize=12, pad=20)\n",
    "ax.grid(axis='x', linestyle='--', alpha=0.7)  # Add grid lines for readability\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### 3- OSs with over 500 VMs Associated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Filter and display OSs with over 500 VMs\n",
    "over_500_vms = inscope_guest_os_pivot[inscope_guest_os_pivot['VM'] > 500]\n",
    "display(over_500_vms)\n",
    "\n",
    "# Check if there are any OSs with more than 500 VMs\n",
    "if over_500_vms.empty:\n",
    "    print(\"No operating systems have more than 500 VMs.\")\n",
    "else:\n",
    "    # Pie chart for OSs with over 500 VMs\n",
    "    plt.figure(figsize=(10, 8))  # Slightly wider for better readability\n",
    "\n",
    "    # Define a color palette\n",
    "    colors = plt.cm.tab10.colors[:len(over_500_vms)]\n",
    "\n",
    "    # Generate pie chart\n",
    "    wedges, texts, autotexts = plt.pie(\n",
    "        over_500_vms['VM'],\n",
    "        labels=over_500_vms.index,\n",
    "        autopct='%1.1f%%',\n",
    "        startangle=140,\n",
    "        colors=colors,\n",
    "        wedgeprops={'edgecolor': 'black', 'linewidth': 1}  # Add borders for clarity\n",
    "    )\n",
    "\n",
    "    # Improve text readability\n",
    "    for text in texts:\n",
    "        text.set_fontsize(10)  # Adjust label size\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_fontsize(10)  # Adjust percentage text size\n",
    "        autotext.set_color('white')  # Improve visibility\n",
    "\n",
    "    # Add title\n",
    "    plt.title('Operating Systems with Over 500 VMs', fontsize=12, pad=20)\n",
    "\n",
    "    # Adjust layout to prevent labels from overlapping\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "### 4- Categorize Host Compute Nodes\n",
    "\n",
    "Categorize ALL the compute nodes by the model number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group all hosts by model\n",
    "all_host_model_pivot_df = consolidated_vhost_df.pivot_table(index=['Vendor', 'Model'], values='Host', aggfunc='count')\n",
    "display(all_host_model_pivot_df)\n",
    "\n",
    "# Bar chart for all host models\n",
    "plt.figure(figsize=(12, 8))  # Make it wider to avoid label overlap\n",
    "\n",
    "# Create a bar chart\n",
    "bar_plot = all_host_model_pivot_df['Host'].plot(kind='bar', color=plt.cm.tab10.colors[:len(all_host_model_pivot_df)], edgecolor='black', figsize=(12, 8))\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('All Host Node Compute Models', fontsize=12)\n",
    "plt.xlabel('Host Model', fontsize=10)\n",
    "plt.ylabel('Host Count', fontsize=10)\n",
    "\n",
    "# Improve x-tick readability by rotating the labels\n",
    "plt.xticks(rotation=90, ha='right', fontsize=8)\n",
    "\n",
    "# Show the bar chart\n",
    "plt.tight_layout()  # Adjust layout to prevent overlapping labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### 5- Group the VMs by disk-size tier\n",
    "\n",
    "This section groups the **in-scope** VMs into categories defined by allocated disk-size tiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Convert disk size to TB and categorize into tiers\n",
    "mib_to_tb_conversion_factor = 2**20 / 10**12\n",
    "inscope_vinfo_df['Disk Size TB'] = inscope_vinfo_df['Total disk capacity MiB'] * mib_to_tb_conversion_factor\n",
    "\n",
    "# Define disk size bins and labels\n",
    "disk_size_bins = [0, 2, 10, 20, 40, 100, float('inf')]\n",
    "disk_bin_labels = ['Easy (<=2 TB)', 'Medium (<=10 TB)', 'Hard (<=20 TB)', 'Very Hard (<=40 TB)', 'Extremely Hard (<=100 TB)', '>100 TB']\n",
    "inscope_vinfo_df['Disk Size Tiers'] = pd.cut(inscope_vinfo_df['Disk Size TB'], bins=disk_size_bins, labels=disk_bin_labels)\n",
    "\n",
    "# Create a pivot table based on disk size tiers\n",
    "disk_tier_pivot_df = inscope_vinfo_df.pivot_table(\n",
    "    index='Disk Size Tiers', \n",
    "    values=['VM', 'Disk Size TB'], \n",
    "    aggfunc={'VM': 'count', 'Disk Size TB': 'sum'}, \n",
    "    observed=False  # Prevent future warning and maintain current behavior\n",
    ")\n",
    "\n",
    "# Add total row with rounded 'Disk Size TB' to nearest 0.5 TB\n",
    "total_row = pd.DataFrame({\n",
    "    'VM': [disk_tier_pivot_df['VM'].sum()],\n",
    "    'Disk Size TB': [round(disk_tier_pivot_df['Disk Size TB'].sum() * 2) / 2]  # Rounding to nearest 0.5 TB\n",
    "}, index=['Total'])\n",
    "\n",
    "# Combine the pivot table with the total row\n",
    "disk_tier_pivot_with_total = pd.concat([disk_tier_pivot_df, total_row])\n",
    "\n",
    "# Format the table for display\n",
    "formatted_table = disk_tier_pivot_with_total.copy()\n",
    "formatted_table['Disk Size TB'] = formatted_table['Disk Size TB'].apply(lambda x: f\"{x:,.2f}\")\n",
    "formatted_table['VM'] = formatted_table['VM'].apply(lambda x: f\"{int(x):,}\")\n",
    "\n",
    "# Display the table\n",
    "print(\"Disk Tier Summary with Total (Formatted):\")\n",
    "print(formatted_table.to_string())\n",
    "\n",
    "# Calculate percentages for VM distribution in each disk tier\n",
    "total_vms = disk_tier_pivot_df['VM'].sum()\n",
    "percentages = (disk_tier_pivot_df['VM'] / total_vms) * 100\n",
    "\n",
    "# Generate labels for the legend\n",
    "labels = [f\"{tier} - {pct:.1f}% ({count} VMs)\" for tier, pct, count in zip(disk_tier_pivot_df.index, percentages, disk_tier_pivot_df['VM'])]\n",
    "\n",
    "# Define colors for the pie chart (green, yellow, red scheme)\n",
    "tier_colors = ['green', 'yellow', 'orange', 'red', 'darkred', 'gray']\n",
    "\n",
    "# Pie Chart 1: Standard Pie Chart with Legend\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(disk_tier_pivot_df['VM'], startangle=140, colors=plt.cm.Paired.colors, wedgeprops={'edgecolor': 'black', 'linewidth': 1})\n",
    "plt.title('VM Distribution by Disk Tier for In-Scope VMs (With Legend)', fontsize=12)\n",
    "plt.legend(labels, loc=\"center left\", bbox_to_anchor=(1, 0, 0.5, 1))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Pie Chart 2: Pie Chart with Green, Yellow, Red Scheme (No Labels on Slices)\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(disk_tier_pivot_df['VM'], colors=tier_colors, autopct='%1.1f%%', startangle=140, wedgeprops={'edgecolor': 'black', 'linewidth': 1})\n",
    "plt.title('VM Distribution by Tier (Easy, Medium, Hard, etc...)', fontsize=12)\n",
    "plt.legend(labels, loc=\"center left\", bbox_to_anchor=(1, 0, 0.5, 1))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "### 6- Categorize Host Compute Nodes by vCenter\n",
    "\n",
    "Categorize all the compute nodes by the model number for each vCenter separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Group all the hosts by their model and vCenter\n",
    "all_host_model_vcenter_pivot_df = consolidated_vhost_df.pivot_table(\n",
    "    index=['vCenter', 'Vendor', 'Model'],\n",
    "    values=['Host'],\n",
    "    aggfunc={'Host': 'count'},\n",
    "    observed=False,\n",
    "    margins=False,\n",
    "    sort=True\n",
    ")\n",
    "\n",
    "print(f'Distribution of ALL host models by vCenter:\\n')\n",
    "print(all_host_model_vcenter_pivot_df)\n",
    "all_host_model_vcenter_pivot_df.to_clipboard(excel=True)\n",
    "\n",
    "# Bar chart creation based on the pivot table (sorted by host count)\n",
    "plt.figure(figsize=(12, 8))  # Increase figure size for better visibility\n",
    "\n",
    "# Prepare labels and data\n",
    "labels = all_host_model_vcenter_pivot_df.index.map(lambda x: f'{x[1]} {x[2]} ({x[0]})')  # Combine Vendor, Model, and vCenter in the label\n",
    "sizes = all_host_model_vcenter_pivot_df['Host']\n",
    "\n",
    "# Choose a color palette\n",
    "colors = plt.cm.tab20(np.linspace(0, 1, len(sizes)))\n",
    "\n",
    "# Create the bar chart\n",
    "plt.barh(labels, sizes, color=colors, edgecolor='black')  # Horizontal bar chart\n",
    "\n",
    "# Add title and formatting\n",
    "plt.title('In-Scope Host Node Compute Models', fontsize=12)\n",
    "plt.xlabel('Number of Hosts', fontsize=10)\n",
    "plt.ylabel('Host Model', fontsize=10)\n",
    "\n",
    "# Display the bar chart\n",
    "plt.tight_layout()  # Adjust layout to prevent clipping\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "### 7- Count the ESXi Clusters with In-Scope VMs\n",
    "\n",
    "Count the total ESXi clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique Datacenters\n",
    "datacenters = filtered_vinfo_df['Datacenter'].unique()\n",
    "\n",
    "# Print total number of in-scope VMware Datacenters\n",
    "print(f'Total in-scope VMware Datacenters: {len(datacenters):,}')\n",
    "\n",
    "# Get unique Clusters\n",
    "clusters = filtered_vinfo_df['Cluster'].unique()\n",
    "\n",
    "# Print total number of in-scope ESXi clusters\n",
    "print(f'Total in-scope ESXi clusters: {len(clusters):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### 8- VM distribution by ESXi Clusters\n",
    "\n",
    "This is orthogonal to the VM distribution analysis by `vCenters`, as a vCenter is likely to contain multiple `clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the VM information on the clusters\n",
    "inscope_cluster_pivot = inscope_vinfo_df.pivot_table(index='Cluster',\n",
    "                                                      values=['VM'],\n",
    "                                                      aggfunc={'VM': 'count'},\n",
    "                                                      sort=True)\n",
    "\n",
    "# Calculate the total number of VMs\n",
    "total_vms = inscope_cluster_pivot['VM'].sum()\n",
    "\n",
    "# Calculate the total number of clusters\n",
    "total_clusters = inscope_cluster_pivot.index.nunique()\n",
    "\n",
    "print(f'Distribution In-Scope VMs to Clusters:\\n')\n",
    "print(inscope_cluster_pivot)\n",
    "print(f'\\nTotal VMs across all clusters: {total_vms}')\n",
    "print(f'Total number of clusters: {total_clusters}')\n",
    "inscope_cluster_pivot.to_clipboard(excel=True)\n",
    "\n",
    "# Create a pie chart\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(inscope_cluster_pivot['VM'], labels=inscope_cluster_pivot.index, autopct='%1.1f%%', startangle=140)\n",
    "plt.title('VMs distributed to clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "### 9- VMs Categorized by Environment\n",
    "\n",
    "This analysis is separate from the VM distribution by environment. The goal is to enhance the vInfo data with additional categorization.\n",
    "\n",
    "1. Create an Environment Column: This column will be based on the name of the ESXi cluster, which indicates the site location.\n",
    "2. Add a Site-Type Column: This column will categorize the environment into one of the following types: Prod, QA, Test, or NonProd.\n",
    "3. Handle Unclassified Clusters: Any cluster names that do not fall into the four categories above will be grouped as \"Unknown.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### VMs Categorized by Environment\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a function to categorize 'Cluster' into environments based on 'Environment' column or 'Cluster' name\n",
    "def determine_environment(row):\n",
    "    # Ensure 'Environment' column is checked first\n",
    "    environment = str(row.get('Environment', '')).strip().lower()\n",
    "    if environment in ['nonprod', 'non-production', 'nonproduction']:\n",
    "        return 'NonProd'\n",
    "    elif environment in ['prod', 'production']:\n",
    "        return 'Prod'\n",
    "    elif environment in ['dev', 'development']:\n",
    "        return 'Dev'\n",
    "    elif environment in ['qa']:\n",
    "        return 'QA'\n",
    "    elif environment in ['test']:\n",
    "        return 'Test'\n",
    "\n",
    "    # If 'Environment' is empty, check 'Cluster' name for environment keywords\n",
    "    cluster_name = str(row.get('Cluster', '')).strip().lower()\n",
    "    if 'nonprod' in cluster_name:\n",
    "        return 'NonProd'\n",
    "    elif 'prod' in cluster_name:\n",
    "        return 'Prod'\n",
    "    elif 'dev' in cluster_name:\n",
    "        return 'Dev'\n",
    "    elif 'qa' in cluster_name:\n",
    "        return 'QA'\n",
    "    elif 'test' in cluster_name:\n",
    "        return 'Test'\n",
    "    \n",
    "    # Return 'Unknown' if no matching environment is found\n",
    "    return 'Unknown'\n",
    "\n",
    "# Apply the function to create or update the 'Environment' column in the DataFrame\n",
    "inscope_vinfo_df['Environment'] = inscope_vinfo_df.apply(determine_environment, axis=1)\n",
    "\n",
    "# Display the distribution of environments\n",
    "print(f\"Environment Distribution:\\n\")\n",
    "environment_counts = inscope_vinfo_df['Environment'].value_counts()\n",
    "for env, count in environment_counts.items():\n",
    "    print(f\"{env}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "### 10- VMs Graphed by Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Block Display by Environment\n",
    "\n",
    "# Count the VMs classified by each environment type\n",
    "print(f\"Total In-scope VMs in Prod   : {len(inscope_vinfo_df[inscope_vinfo_df['Environment'] == 'Prod']):,}\")\n",
    "print(f\"Total In-scope VMs in Dev    : {len(inscope_vinfo_df[inscope_vinfo_df['Environment'] == 'Dev']):,}\")\n",
    "print(f\"Total In-scope VMs in QA     : {len(inscope_vinfo_df[inscope_vinfo_df['Environment'] == 'QA']):,}\")\n",
    "print(f\"Total In-scope VMs in Test   : {len(inscope_vinfo_df[inscope_vinfo_df['Environment'] == 'Test']):,}\")\n",
    "print(f\"Total In-scope VMs in NonProd : {len(inscope_vinfo_df[inscope_vinfo_df['Environment'] == 'NonProd']):,}\")\n",
    "print(f\"Total In-scope VMs in Unknown: {len(inscope_vinfo_df[inscope_vinfo_df['Environment'] == 'Unknown']):,}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Count the number of VMs in each environment type\n",
    "env_counts = inscope_vinfo_df['Environment'].value_counts()\n",
    "\n",
    "# Plot the pie chart\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(env_counts, labels=env_counts.index, autopct='%1.1f%%', startangle=140, colors=plt.cm.tab20.colors, wedgeprops={'edgecolor': 'black'})\n",
    "plt.title('VM Distribution by Environment')\n",
    "\n",
    "# Display the pie chart\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie chart is drawn as a circle.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "### 10- Summarize Operating Systems by Supported vs. Unsupported\n",
    "\n",
    "In this section, we provide a summary of the supported and unsupported operating systems for the in-scope vCenter instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Define the list of supported OSs for categorization\n",
    "supported_os_list = [\n",
    "    \"Red Hat Enterprise Linux 7 (64-bit)\", \"Red Hat Enterprise Linux 8 (64-bit)\", \"Red Hat Enterprise Linux 9 (64-bit)\",\n",
    "    \"Microsoft Windows 10 (64-bit)\", \"Microsoft Windows 11 (64-bit)\", \"Microsoft Windows Server 2012 (64-bit)\",\n",
    "    \"Microsoft Windows Server 2016 (64-bit)\", \"Microsoft Windows Server 2016 or later (64-bit)\", \n",
    "    \"Microsoft Windows Server 2019 (64-bit)\", \"Microsoft Windows Server 2022 (64-bit)\", \n",
    "    \"SUSE Linux Enterprise 15 (64-bit)\", \"SUSE Linux Enterprise 12 (64-bit)\", \n",
    "    \"Ubuntu 18.04 (64-bit)\", \"Ubuntu 20.04 (64-bit)\", \"Ubuntu 22.04 (64-bit)\", \n",
    "    \"Ubuntu 24.04 (64-bit)\", \"Fedora 30 (64-bit)\", \"Fedora 40 (64-bit)\", \n",
    "    \"CentOS 9 (64-bit)\", \"CentOS 8 (64-bit)\"\n",
    "]\n",
    "\n",
    "# Combine the OS columns into one \"Final OS\"\n",
    "inscope_vinfo_df['Final OS'] = inscope_vinfo_df['OS according to the VMware Tools'].combine_first(inscope_vinfo_df['OS according to the configuration file'])\n",
    "\n",
    "# Separate supported and unsupported OS based on \"Final OS\"\n",
    "supported_final_df = inscope_vinfo_df[inscope_vinfo_df['Final OS'].isin(supported_os_list)]\n",
    "unsupported_final_df = inscope_vinfo_df[~inscope_vinfo_df['Final OS'].isin(supported_os_list)]\n",
    "\n",
    "# Create pivot tables for supported and unsupported OS\n",
    "supported_final_pivot = supported_final_df.pivot_table(index='Final OS', values='VM', aggfunc='count')\n",
    "unsupported_final_pivot = unsupported_final_df.pivot_table(index='Final OS', values='VM', aggfunc='count')\n",
    "\n",
    "# Add totals to the pivot tables\n",
    "supported_final_total = supported_final_pivot['VM'].sum()\n",
    "unsupported_final_total = unsupported_final_pivot['VM'].sum()\n",
    "supported_final_pivot.loc['Total'] = supported_final_total\n",
    "unsupported_final_pivot.loc['Total'] = unsupported_final_total\n",
    "\n",
    "# Display the pivot tables\n",
    "print(\"Supported Operating Systems (Final OS) with VM Counts:\")\n",
    "display(supported_final_pivot)\n",
    "print(\"\\nUnsupported Operating Systems (Final OS) with VM Counts:\")\n",
    "display(unsupported_final_pivot)\n",
    "\n",
    "# Plotting supported and unsupported OS bar charts\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(supported_final_pivot.index, supported_final_pivot['VM'], color='green')\n",
    "plt.xlabel('Supported Operating Systems (Final OS)')\n",
    "plt.ylabel('Number of VMs')\n",
    "plt.title('Supported Operating Systems with VM Counts (Final OS)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(unsupported_final_pivot.index, unsupported_final_pivot['VM'], color='red')\n",
    "plt.xlabel('Unsupported Operating Systems (Final OS)')\n",
    "plt.ylabel('Number of VMs')\n",
    "plt.title('Unsupported Operating Systems with VM Counts (Final OS)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Process VMware Tools and Configuration File OS columns similarly\n",
    "def process_os_column(os_column_name):\n",
    "    supported_df = inscope_vinfo_df[inscope_vinfo_df[os_column_name].isin(supported_os_list)]\n",
    "    unsupported_df = inscope_vinfo_df[~inscope_vinfo_df[os_column_name].isin(supported_os_list)]\n",
    "    \n",
    "    supported_pivot = supported_df.pivot_table(index=os_column_name, values='VM', aggfunc='count')\n",
    "    unsupported_pivot = unsupported_df.pivot_table(index=os_column_name, values='VM', aggfunc='count')\n",
    "    \n",
    "    # Add totals\n",
    "    supported_total = supported_pivot['VM'].sum()\n",
    "    unsupported_total = unsupported_pivot['VM'].sum()\n",
    "    supported_pivot.loc['Total'] = supported_total\n",
    "    unsupported_pivot.loc['Total'] = unsupported_total\n",
    "    \n",
    "    return supported_pivot, unsupported_pivot\n",
    "\n",
    "# Process for \"OS according to VMware Tools\"\n",
    "supported_vmware_tools_pivot, unsupported_vmware_tools_pivot = process_os_column('OS according to the VMware Tools')\n",
    "\n",
    "# Display and plot VMware Tools OS counts\n",
    "print(\"\\nSupported Operating Systems (VMware Tools) with VM Counts:\")\n",
    "display(supported_vmware_tools_pivot)\n",
    "print(\"\\nUnsupported Operating Systems (VMware Tools) with VM Counts:\")\n",
    "display(unsupported_vmware_tools_pivot)\n",
    "\n",
    "# Plot VMware Tools bar charts\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(supported_vmware_tools_pivot.index, supported_vmware_tools_pivot['VM'], color='green')\n",
    "plt.xlabel('Supported Operating Systems (VMware Tools)')\n",
    "plt.ylabel('Number of VMs')\n",
    "plt.title('Supported Operating Systems with VM Counts (VMware Tools)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(unsupported_vmware_tools_pivot.index, unsupported_vmware_tools_pivot['VM'], color='red')\n",
    "plt.xlabel('Unsupported Operating Systems (VMware Tools)')\n",
    "plt.ylabel('Number of VMs')\n",
    "plt.title('Unsupported Operating Systems with VM Counts (VMware Tools)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Process for \"OS according to the Configuration File\"\n",
    "supported_config_file_pivot, unsupported_config_file_pivot = process_os_column('OS according to the configuration file')\n",
    "\n",
    "# Display and plot Configuration File OS counts\n",
    "print(\"\\nSupported Operating Systems (Configuration File) with VM Counts:\")\n",
    "display(supported_config_file_pivot)\n",
    "print(\"\\nUnsupported Operating Systems (Configuration File) with VM Counts:\")\n",
    "display(unsupported_config_file_pivot)\n",
    "\n",
    "# Plot Configuration File bar charts\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(supported_config_file_pivot.index, supported_config_file_pivot['VM'], color='green')\n",
    "plt.xlabel('Supported Operating Systems (Configuration File)')\n",
    "plt.ylabel('Number of VMs')\n",
    "plt.title('Supported Operating Systems with VM Counts (Configuration File)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(unsupported_config_file_pivot.index, unsupported_config_file_pivot['VM'], color='red')\n",
    "plt.xlabel('Unsupported Operating Systems (Configuration File)')\n",
    "plt.ylabel('Number of VMs')\n",
    "plt.title('Unsupported Operating Systems with VM Counts (Configuration File)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "### 11- Migration Complexity In-Scope VMs by OS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Define the list of supported OSs and specific non-supported OSes\n",
    "supported_os_list = [\n",
    "    \"Red Hat Enterprise Linux 7 (64-bit)\", \"Red Hat Enterprise Linux 8 (64-bit)\", \"Red Hat Enterprise Linux 9 (64-bit)\",\n",
    "    \"Microsoft Windows 10 (64-bit)\", \"Microsoft Windows 11 (64-bit)\", \n",
    "    \"Microsoft Windows Server 2016 (64-bit)\", \"Microsoft Windows Server 2019 (64-bit)\", \n",
    "    \"Microsoft Windows Server 2022 (64-bit)\", \"Microsoft Windows Server 2016 or later (64-bit)\",\n",
    "    \"SUSE Linux Enterprise 15 (64-bit)\", \"SUSE Linux Enterprise 12 (64-bit)\", \n",
    "    \"Ubuntu 18.04 (64-bit)\", \"Ubuntu 20.04 (64-bit)\", \"Ubuntu 22.04 (64-bit)\", \n",
    "    \"Ubuntu 24.04 (64-bit)\", \"Fedora 30 (64-bit)\", \"Fedora 40 (64-bit)\", \n",
    "    \"CentOS 9 (64-bit)\", \"CentOS 8 (64-bit)\"\n",
    "]\n",
    "\n",
    "easy_os_list = [\n",
    "    \"Oracle Linux 7 (64-bit)\", \"Oracle Linux 8 (64-bit)\", \"Oracle Linux 9 (64-bit)\",\n",
    "    \"CentOS 7 (64-bit)\", \"Red Hat Enterprise Linux 6 (64-bit)\"\n",
    "]\n",
    "\n",
    "# Create dictionaries for exact lookups\n",
    "supported_os_dict = {os: True for os in supported_os_list}\n",
    "easy_os_dict = {os: True for os in easy_os_list}\n",
    "\n",
    "# Pre-process the Final OS column\n",
    "inscope_vinfo_df['Final OS'] = inscope_vinfo_df['Final OS'].fillna(\"Unknown OS\").astype(str).str.strip()\n",
    "\n",
    "# Preprocess the Cluster column\n",
    "inscope_vinfo_df['Cluster'] = inscope_vinfo_df['Cluster'].fillna('').str.lower()\n",
    "\n",
    "# Function to classify Database Group (SQL/Oracle/Other)\n",
    "def classify_database_group(cluster_name):\n",
    "    if 'sql-' in cluster_name:\n",
    "        return 'SQL'\n",
    "    elif 'ora' in cluster_name:\n",
    "        return 'Oracle'\n",
    "    return 'Other'\n",
    "\n",
    "inscope_vinfo_df['Database Group'] = inscope_vinfo_df['Cluster'].apply(classify_database_group)\n",
    "\n",
    "# Classification function\n",
    "def classify_disk_size(disk_size_tb, os_name, network, database_group):\n",
    "    os_name_stripped = os_name.strip()\n",
    "    \n",
    "    # Database-specific groupings\n",
    "    if database_group == 'SQL':\n",
    "        base_group = \"SQL-DBs - \"\n",
    "    elif database_group == 'Oracle':\n",
    "        base_group = \"Oracle-DBs - \"\n",
    "    else:\n",
    "        base_group = \"\"\n",
    "    \n",
    "    # Check if Network contains F5\n",
    "    if 'F5' in str(network).upper():\n",
    "        return f\"{base_group}F5-Hard\"\n",
    "    \n",
    "    # Easy criteria\n",
    "    if disk_size_tb <= 2 and (easy_os_dict.get(os_name_stripped) or (supported_os_dict.get(os_name_stripped) and not os_name_stripped.startswith(\"Microsoft\"))):\n",
    "        if easy_os_dict.get(os_name_stripped):\n",
    "            return f\"{base_group}Easy - Unsupported\"\n",
    "        return f\"{base_group}Easy\"\n",
    "    \n",
    "    # Medium criteria\n",
    "    if (2 < disk_size_tb <= 10 and (supported_os_dict.get(os_name_stripped) or easy_os_dict.get(os_name_stripped))) or \\\n",
    "       (disk_size_tb <= 2 and os_name_stripped.startswith(\"Microsoft\") and supported_os_dict.get(os_name_stripped)):\n",
    "        if easy_os_dict.get(os_name_stripped):\n",
    "            return f\"{base_group}Medium - Unsupported\"\n",
    "        return f\"{base_group}Medium\"\n",
    "    \n",
    "    # Hard criteria\n",
    "    if disk_size_tb > 10 and (supported_os_dict.get(os_name_stripped) or easy_os_dict.get(os_name_stripped)):\n",
    "        if easy_os_dict.get(os_name_stripped):\n",
    "            return f\"{base_group}Hard - Unsupported\"\n",
    "        return f\"{base_group}Hard\"\n",
    "    \n",
    "    # Hard - OS Unsupported\n",
    "    return f\"{base_group}Hard - OS Unsupported\"\n",
    "\n",
    "# Apply the classification logic\n",
    "inscope_vinfo_df['Disk Classification'] = inscope_vinfo_df.apply(\n",
    "    lambda row: classify_disk_size(row['Disk Size TB'], row['Final OS'], row['Network #1'], row['Database Group']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Combine unsupported groups into \"Unsupported OS\"\n",
    "def combine_unsupported_classifications(classification):\n",
    "    if \"Unsupported\" in classification:\n",
    "        if \"Hard\" in classification:\n",
    "            return \"Hard - Unsupported OS\"\n",
    "        elif \"Medium\" in classification:\n",
    "            return \"Medium - Unsupported OS\"\n",
    "        elif \"Easy\" in classification:\n",
    "            return \"Easy - Unsupported OS\"\n",
    "    return classification\n",
    "\n",
    "inscope_vinfo_df['Combined Classification'] = inscope_vinfo_df['Disk Classification'].apply(combine_unsupported_classifications)\n",
    "\n",
    "# Summarize counts by combined classification\n",
    "disk_classification_summary = inscope_vinfo_df.pivot_table(\n",
    "    index='Combined Classification',\n",
    "    values='VM',\n",
    "    aggfunc='count',\n",
    "    fill_value=0\n",
    ")\n",
    "disk_classification_summary = disk_classification_summary[disk_classification_summary['VM'] > 0]\n",
    "\n",
    "# Assign colors based on classification\n",
    "classification_colors = {\n",
    "    'Easy': 'green',\n",
    "    'Medium': 'yellow',\n",
    "    'Hard': 'red',\n",
    "    'Unsupported OS': 'gray'\n",
    "}\n",
    "\n",
    "def get_color_for_classification(classification):\n",
    "    if \"Easy\" in classification:\n",
    "        return classification_colors['Easy']\n",
    "    elif \"Medium\" in classification:\n",
    "        return classification_colors['Medium']\n",
    "    elif \"Hard\" in classification:\n",
    "        return classification_colors['Hard']\n",
    "    return classification_colors.get(\"Unsupported OS\", 'gray')  # Default to gray for unsupported OS\n",
    "\n",
    "colors = [get_color_for_classification(cls) for cls in disk_classification_summary.index]\n",
    "\n",
    "# Plot the bar chart with color coding\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(disk_classification_summary.index, disk_classification_summary['VM'], color=colors)\n",
    "plt.xlabel('Classification')\n",
    "plt.ylabel('Number of VMs')\n",
    "plt.title('VM Distribution by Classification')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Extract unsupported OS breakdown\n",
    "unsupported_combined_df = inscope_vinfo_df[\n",
    "    inscope_vinfo_df['Disk Classification'].str.contains(\"Unsupported\", na=False)\n",
    "].drop_duplicates(subset=['VM'])\n",
    "\n",
    "unsupported_os_summary = unsupported_combined_df.groupby('Final OS').size().reset_index(name='VM Count')\n",
    "unsupported_os_summary = unsupported_os_summary[unsupported_os_summary['VM Count'] > 0]\n",
    "\n",
    "# Print unsupported OS breakdown\n",
    "if not unsupported_os_summary.empty:\n",
    "    print(\"\\nUnsupported OS Breakdown:\")\n",
    "    print(unsupported_os_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "### 12- Migration Complexity In-Scope VMs by Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assume 'inscope_vinfo_df' contains all required data, including 'Environment' and 'Disk Size TB'\n",
    "\n",
    "# Define disk size categories\n",
    "disk_bins = [0, 2, float('inf')]\n",
    "disk_labels = ['<= 2TB', '> 2TB']\n",
    "\n",
    "# Create a column to categorize disk size into <= 2TB and > 2TB\n",
    "inscope_vinfo_df['Disk Size Category'] = pd.cut(inscope_vinfo_df['Disk Size TB'], bins=disk_bins, labels=disk_labels)\n",
    "\n",
    "# Create a pivot table to get VM totals for each environment and disk size category\n",
    "env_disk_summary = inscope_vinfo_df.pivot_table(\n",
    "    index='Environment',\n",
    "    columns='Disk Size Category',\n",
    "    values='VM',\n",
    "    aggfunc='count',\n",
    "    fill_value=0,\n",
    "    observed=True  # Explicitly specify observed=True\n",
    ").reset_index()\n",
    "\n",
    "# Dynamically rename columns based on the actual columns in the pivot table\n",
    "env_disk_summary.columns = ['Environment'] + [col if isinstance(col, str) else col[1] for col in env_disk_summary.columns[1:]]\n",
    "\n",
    "# Display the summary table for each environment with VM totals by disk size category\n",
    "print(\"VM Totals by Environment and Disk Size Category:\")\n",
    "print(env_disk_summary.to_string(index=False, header=True))\n",
    "\n",
    "# Plotting (optional)\n",
    "\n",
    "# Bar chart for VM Disk Size Totals by Environment\n",
    "env_disk_summary.plot(\n",
    "    x='Environment', \n",
    "    kind='bar', \n",
    "    stacked=False, \n",
    "    figsize=(10, 6), \n",
    "    width=0.8\n",
    ")\n",
    "plt.xlabel('Environment')\n",
    "plt.ylabel('Number of VMs')\n",
    "plt.title('VM Totals by Environment and Disk Size Category')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "### 13- Estimated Migration Time by OS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Constants\n",
    "migration_time_per_500gb = 110  # minutes (1 hour 50 minutes per 500GB)\n",
    "fte_hours_per_day = 8           # 8 hours per day per FTE\n",
    "fte_count = 10                  # 10 FTEs available\n",
    "pmt_hours = 1.5                 # Post-migration troubleshooting time per VM in hours\n",
    "\n",
    "# Example list of supported operating systems (uncomment and modify as needed)\n",
    "supported_os_list = ['Windows Server', 'Red Hat', 'Oracle Linux']  # Add your OS list here\n",
    "\n",
    "# Ensure required columns are present in the existing inscope_vinfo_df\n",
    "required_columns = ['Environment', 'Final OS', 'Disk Size TB', 'VM', 'Cluster', 'Disk Classification']\n",
    "missing_columns = [col for col in required_columns if col not in inscope_vinfo_df.columns]\n",
    "\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"The following required columns are missing in inscope_vinfo_df: {missing_columns}\")\n",
    "\n",
    "# Preprocess columns\n",
    "inscope_vinfo_df['Final OS'] = inscope_vinfo_df['Final OS'].fillna(\"Unknown OS\").astype(str).str.strip()\n",
    "inscope_vinfo_df['Environment'] = inscope_vinfo_df['Environment'].fillna('unknown').str.lower().str.strip()\n",
    "inscope_vinfo_df['Disk Size TB'] = pd.to_numeric(inscope_vinfo_df['Disk Size TB'], errors='coerce')\n",
    "inscope_vinfo_df['VM'] = inscope_vinfo_df['VM'].fillna('Unknown VM')\n",
    "inscope_vinfo_df['Cluster'] = inscope_vinfo_df['Cluster'].fillna('').str.lower()\n",
    "\n",
    "# Derive F5-Hard complexity directly from Disk Classification\n",
    "inscope_vinfo_df['Complexity'] = inscope_vinfo_df['Disk Classification'].apply(\n",
    "    lambda x: 'F5-Hard' if 'F5-Hard' in str(x) else None\n",
    ")\n",
    "\n",
    "# Assign other complexities only for non-F5-Hard rows\n",
    "def classify_complexity(row):\n",
    "    if row['Complexity'] == 'F5-Hard':  # Skip if already classified as F5-Hard\n",
    "        return 'F5-Hard'\n",
    "    if 'sql-' in row['Cluster']:\n",
    "        return 'MSSQL-DBs'  # Changed from SQL-DBs to MSSQL-DBs\n",
    "    elif 'ora' in row['Cluster']:\n",
    "        return 'Oracle-DBs'\n",
    "    elif row['Disk Size TB'] <= 2:\n",
    "        return 'Easy'\n",
    "    elif row['Disk Size TB'] <= 5:\n",
    "        return 'Medium'\n",
    "    elif row['Disk Size TB'] <= 10:\n",
    "        return 'Hard'\n",
    "    else:\n",
    "        return 'Hard - Very Large'\n",
    "\n",
    "inscope_vinfo_df['Complexity'] = inscope_vinfo_df.apply(classify_complexity, axis=1)\n",
    "\n",
    "# Define the custom sorting order for Complexity\n",
    "complexity_order = ['Easy', 'Medium', 'Hard', 'F5-Hard', 'Hard - Very Large', 'Oracle-DBs', 'MSSQL-DBs']\n",
    "inscope_vinfo_df['Complexity'] = pd.Categorical(\n",
    "    inscope_vinfo_df['Complexity'],\n",
    "    categories=complexity_order,\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "# Sort the DataFrame by Complexity\n",
    "inscope_vinfo_df = inscope_vinfo_df.sort_values('Complexity')\n",
    "\n",
    "# Create the OS Support column based on supported_os_list\n",
    "inscope_vinfo_df['OS Support'] = inscope_vinfo_df['Final OS'].apply(\n",
    "    lambda os: 'Supported' if any(supported_os in os for supported_os in supported_os_list) else 'Not Supported'\n",
    ")\n",
    "\n",
    "# Add Migration Time\n",
    "inscope_vinfo_df['Migration Time (minutes)'] = inscope_vinfo_df['Disk Size TB'].apply(\n",
    "    lambda size: ((size * 1024) / 500) * migration_time_per_500gb\n",
    ")\n",
    "\n",
    "# Add Post-Migration Troubleshooting Time\n",
    "pmt_minutes = pmt_hours * 60\n",
    "inscope_vinfo_df['Total Time (minutes)'] = inscope_vinfo_df['Migration Time (minutes)'] + pmt_minutes\n",
    "\n",
    "# Summarize by Complexity and OS Support\n",
    "disk_classification_summary = inscope_vinfo_df.groupby(['Complexity', 'OS Support'], observed=True).agg(\n",
    "    VM_Count=('VM', 'count'),\n",
    "    Total_Disk=('Disk Size TB', 'sum'),\n",
    "    Total_Mig_Time_Minutes=('Total Time (minutes)', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Include all \"OS Support\" categories in the summary\n",
    "for complexity in complexity_order:\n",
    "    for os_support in ['Supported', 'Not Supported']:\n",
    "        if not ((disk_classification_summary['Complexity'] == complexity) &\n",
    "                (disk_classification_summary['OS Support'] == os_support)).any():\n",
    "            disk_classification_summary = pd.concat([\n",
    "                disk_classification_summary,\n",
    "                pd.DataFrame({\n",
    "                    'Complexity': [complexity],\n",
    "                    'OS Support': [os_support],\n",
    "                    'VM_Count': [0],\n",
    "                    'Total_Disk': [0],\n",
    "                    'Total_Mig_Time_Minutes': [0]\n",
    "                })\n",
    "            ], ignore_index=True)\n",
    "\n",
    "# Filter out rows where VM_Count is 0\n",
    "disk_classification_summary = disk_classification_summary[disk_classification_summary['VM_Count'] > 0]\n",
    "\n",
    "# Formatting for Migration Time and Days Calculation (1 decimal place)\n",
    "disk_classification_summary['Formatted_Mig_Time'] = disk_classification_summary['Total_Mig_Time_Minutes'].apply(\n",
    "    lambda minutes: f\"{minutes / 60:,.1f}h\"  # Converted minutes to hours with 1 decimal place\n",
    ")\n",
    "disk_classification_summary['Days_Per_FTEs'] = disk_classification_summary['Total_Mig_Time_Minutes'].apply(\n",
    "    lambda minutes: f\"{minutes / (fte_hours_per_day * 60 * fte_count):,.1f}\"  # 1 decimal place\n",
    ")\n",
    "\n",
    "# Format VM Count with thousands separator\n",
    "disk_classification_summary['VM_Count'] = disk_classification_summary['VM_Count'].apply(lambda x: f\"{x:,}\")\n",
    "\n",
    "# Format Total Disk (TB) without rounding\n",
    "disk_classification_summary['Total_Disk'] = disk_classification_summary['Total_Disk'].apply(lambda x: f\"{x:,.2f}\")\n",
    "\n",
    "# Add totals row\n",
    "totals_row = {\n",
    "    'Complexity': 'Totals',\n",
    "    'OS Support': '',\n",
    "    'VM_Count': f\"{int(disk_classification_summary['VM_Count'].str.replace(',', '').astype(int).sum()):,}\",\n",
    "    'Total_Disk': f\"{float(disk_classification_summary['Total_Disk'].str.replace(',', '').astype(float).sum()):,.2f}\",\n",
    "    'Formatted_Mig_Time': f\"{disk_classification_summary['Total_Mig_Time_Minutes'].sum() / 60:,.1f}h\",\n",
    "    'Days_Per_FTEs': f\"{disk_classification_summary['Total_Mig_Time_Minutes'].sum() / (fte_hours_per_day * 60 * fte_count):,.1f}\"\n",
    "}\n",
    "disk_classification_summary = pd.concat(\n",
    "    [disk_classification_summary, pd.DataFrame([totals_row])],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# Print the table\n",
    "headers = [\n",
    "    \"Complexity\", \"OS Support\", \"VM Count\", \"Total Disk (TB)\",\n",
    "    \"Total Migration Time\", f\"Days ({fte_count} FTEs)\"\n",
    "]\n",
    "rows = disk_classification_summary[\n",
    "    ['Complexity', 'OS Support', 'VM_Count', 'Total_Disk', 'Formatted_Mig_Time', 'Days_Per_FTEs']\n",
    "].values.tolist()\n",
    "\n",
    "def custom_table_format_with_totals(headers, rows):\n",
    "    horizontal_line = \"â”€\"\n",
    "    vertical_line = \"â”‚\"\n",
    "    corner_tl, corner_tr = \"â•­\", \"â•®\"\n",
    "    corner_bl, corner_br = \"â•°\", \"â•¯\"\n",
    "    join_t, join_b, join_c = \"â”¬\", \"â”´\", \"â”¼\"\n",
    "    col_widths = [max(len(str(item)) for item in col) for col in zip(headers, *rows)]\n",
    "    def make_row(items):\n",
    "        return vertical_line + vertical_line.join(f\"{str(item).rjust(width)}\" for item, width in zip(items, col_widths)) + vertical_line\n",
    "    top_line = corner_tl + join_t.join(horizontal_line * width for width in col_widths) + corner_tr\n",
    "    header_row = make_row(headers)\n",
    "    divider_row = join_c.join(horizontal_line * width for width in col_widths).join([\"â”œ\", \"â”¤\"])\n",
    "    data_rows = [make_row(row) for row in rows[:-1]]\n",
    "    totals_divider_row = join_c.join(horizontal_line * width for width in col_widths).join([\"â”œ\", \"â”¤\"])\n",
    "    totals_row = make_row(rows[-1])\n",
    "    bottom_line = corner_bl + join_b.join(horizontal_line * width for width in col_widths) + corner_br\n",
    "    return \"\\n\".join([top_line, header_row, divider_row] + data_rows + [totals_divider_row, totals_row, bottom_line])\n",
    "\n",
    "print(\"\\nSummary Table:\")\n",
    "print(custom_table_format_with_totals(headers, rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3a5090-41a4-48c2-8f84-2afec0cf6089",
   "metadata": {},
   "source": [
    "### 14- Estimated Migration Time by vCenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9334fa83-e0f0-4c23-b4df-af8f97541f69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Constants\n",
    "fte_hours_per_day = 8           # 8 hours per day per FTE\n",
    "fte_count = 10                  # 10 FTEs available\n",
    "complexity_order = ['Easy', 'Medium', 'Hard', 'F5-Hard', 'Hard - Very Large', 'Oracle-DBs', 'MSSQL-DBs']\n",
    "\n",
    "# Assuming 'inscope_vinfo_df' is the DataFrame that already exists\n",
    "vcenters = inscope_vinfo_df['vCenter'].unique()\n",
    "\n",
    "# Create a dictionary to store summaries for each vCenter\n",
    "vcenter_summaries = {}\n",
    "\n",
    "# Function to format the table output\n",
    "def custom_table_format_with_totals(headers, rows):\n",
    "    horizontal_line = \"â”€\"\n",
    "    vertical_line = \"â”‚\"\n",
    "    corner_tl, corner_tr = \"â•­\", \"â•®\"\n",
    "    corner_bl, corner_br = \"â•°\", \"â•¯\"\n",
    "    join_t, join_b, join_c = \"â”¬\", \"â”´\", \"â”¼\"\n",
    "    col_widths = [max(len(str(item)) for item in col) for col in zip(headers, *rows)]\n",
    "    \n",
    "    def make_row(items):\n",
    "        return vertical_line + vertical_line.join(f\"{str(item).rjust(width)}\" for item, width in zip(items, col_widths)) + vertical_line\n",
    "    \n",
    "    top_line = corner_tl + join_t.join(horizontal_line * width for width in col_widths) + corner_tr\n",
    "    header_row = make_row(headers)\n",
    "    divider_row = join_c.join(horizontal_line * width for width in col_widths).join([\"â”œ\", \"â”¤\"])\n",
    "    data_rows = [make_row(row) for row in rows[:-1]]\n",
    "    totals_divider_row = join_c.join(horizontal_line * width for width in col_widths).join([\"â”œ\", \"â”¤\"])\n",
    "    totals_row = make_row(rows[-1])\n",
    "    bottom_line = corner_bl + join_b.join(horizontal_line * width for width in col_widths) + corner_br\n",
    "    return \"\\n\".join([top_line, header_row, divider_row] + data_rows + [totals_divider_row, totals_row, bottom_line])\n",
    "\n",
    "# Process each vCenter separately\n",
    "for vcenter in vcenters:\n",
    "    # Filter the DataFrame for the current vCenter\n",
    "    vcenter_df = inscope_vinfo_df[inscope_vinfo_df['vCenter'] == vcenter]\n",
    "    \n",
    "    # Perform the same calculations as before on the filtered DataFrame\n",
    "    disk_classification_summary = vcenter_df.groupby(['Complexity', 'OS Support'], observed=True).agg(\n",
    "        VM_Count=('VM', 'count'),\n",
    "        Total_Disk=('Disk Size TB', 'sum'),\n",
    "        Total_Mig_Time_Minutes=('Total Time (minutes)', 'sum')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Include all \"OS Support\" categories in the summary\n",
    "    for complexity in complexity_order:\n",
    "        for os_support in ['Supported', 'Not Supported']:\n",
    "            if not ((disk_classification_summary['Complexity'] == complexity) &\n",
    "                    (disk_classification_summary['OS Support'] == os_support)).any():\n",
    "                disk_classification_summary = pd.concat([\n",
    "                    disk_classification_summary,\n",
    "                    pd.DataFrame({\n",
    "                        'Complexity': [complexity],\n",
    "                        'OS Support': [os_support],\n",
    "                        'VM_Count': [0],\n",
    "                        'Total_Disk': [0],\n",
    "                        'Total_Mig_Time_Minutes': [0]\n",
    "                    })\n",
    "                ], ignore_index=True)\n",
    "\n",
    "    # Filter out rows where VM_Count is 0\n",
    "    disk_classification_summary = disk_classification_summary[disk_classification_summary['VM_Count'] > 0]\n",
    "    \n",
    "    # Convert 'Complexity' to a categorical type with the custom order\n",
    "    disk_classification_summary['Complexity'] = pd.Categorical(\n",
    "        disk_classification_summary['Complexity'],\n",
    "        categories=complexity_order,\n",
    "        ordered=True\n",
    "    )\n",
    "\n",
    "    # Sort the summary by the custom complexity order\n",
    "    disk_classification_summary = disk_classification_summary.sort_values('Complexity')\n",
    "\n",
    "    # Add calculated columns for formatted migration time and days (1 decimal place)\n",
    "    disk_classification_summary['Formatted_Mig_Time'] = disk_classification_summary['Total_Mig_Time_Minutes'].apply(\n",
    "        lambda minutes: f\"{minutes / 60:,.1f}h\"  # Exact value in hours with 1 decimal place\n",
    "    )\n",
    "    disk_classification_summary['Days_Per_FTEs'] = disk_classification_summary['Total_Mig_Time_Minutes'].apply(\n",
    "        lambda minutes: f\"{minutes / (fte_hours_per_day * 60 * fte_count):,.1f}\"  # Exact value in days with 1 decimal place\n",
    "    )\n",
    "\n",
    "    # Format VM Count with thousands separator\n",
    "    disk_classification_summary['VM_Count'] = disk_classification_summary['VM_Count'].apply(lambda x: f\"{x:,}\")\n",
    "    disk_classification_summary['Total_Disk'] = disk_classification_summary['Total_Disk'].apply(lambda x: f\"{x:,.0f}\")\n",
    "\n",
    "    # Add totals row\n",
    "    totals_row = {\n",
    "        'Complexity': 'Totals',\n",
    "        'OS Support': '',\n",
    "        'VM_Count': f\"{int(disk_classification_summary['VM_Count'].str.replace(',', '').astype(int).sum()):,}\",\n",
    "        'Total_Disk': f\"{float(disk_classification_summary['Total_Disk'].str.replace(',', '').astype(float).sum()):,.0f}\",\n",
    "        'Formatted_Mig_Time': f\"{disk_classification_summary['Total_Mig_Time_Minutes'].sum() / 60:,.1f}h\",\n",
    "        'Days_Per_FTEs': f\"{disk_classification_summary['Total_Mig_Time_Minutes'].sum() / (fte_hours_per_day * 60 * fte_count):,.1f}\"\n",
    "    }\n",
    "    disk_classification_summary = pd.concat(\n",
    "        [disk_classification_summary, pd.DataFrame([totals_row])],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # Store the summary in the dictionary\n",
    "    vcenter_summaries[vcenter] = disk_classification_summary\n",
    "\n",
    "    # Print the table for the current vCenter\n",
    "    print(f\"\\nSummary Table for vCenter: {vcenter}\")\n",
    "    headers = [\n",
    "        \"Complexity\", \"OS Support\", \"VM Count\", \"Total Disk (TB)\",\n",
    "        \"Total Migration Time\", f\"Days ({fte_count} FTEs)\"\n",
    "    ]\n",
    "    rows = disk_classification_summary[\n",
    "        ['Complexity', 'OS Support', 'VM_Count', 'Total_Disk', 'Formatted_Mig_Time', 'Days_Per_FTEs']\n",
    "    ].values.tolist()\n",
    "    print(custom_table_format_with_totals(headers, rows))\n",
    "\n",
    "# Calculate global totals across all vCenters\n",
    "global_total_mig_time = sum(\n",
    "    summary['Total_Mig_Time_Minutes'].sum() for summary in vcenter_summaries.values()\n",
    ")\n",
    "global_total_days = global_total_mig_time / (fte_hours_per_day * 60 * fte_count)\n",
    "\n",
    "# Calculate aggregate FTE days from vCenter summaries\n",
    "aggregate_total_days = sum(\n",
    "    summary.loc[summary['Complexity'] == 'Totals', 'Days_Per_FTEs'].astype(float).sum() for summary in vcenter_summaries.values()\n",
    ")\n",
    "\n",
    "# Display Global Totals (1 decimal place)\n",
    "print(f\"\\nGlobal Total Migration Time: {global_total_mig_time / 60:,.1f}h\")\n",
    "print(f\"Global Total FTE Days (10 FTEs): {global_total_days:,.1f}\")\n",
    "print(f\"Aggregate FTE Days (10 FTEs): {aggregate_total_days:,.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6977fd21-5e69-4260-9664-1d0d99b9ebb9",
   "metadata": {},
   "source": [
    "### 15- Estimated Migration Time (Summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd9bb6a-587e-4c7b-898d-098d91cbea79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#info\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Constants\n",
    "migration_time_per_500gb = 110  # minutes (1 hour 50 minutes per 500GB)\n",
    "fte_hours_per_day = 8           # 8 hours per day per FTE\n",
    "fte_count = 10                  # 10 FTEs available\n",
    "pmt_hours = 1.5                 # Post-migration troubleshooting time per VM in hours\n",
    "\n",
    "# Example list of supported operating systems\n",
    "supported_os_list = ['Windows Server', 'Red Hat', 'Oracle Linux']\n",
    "\n",
    "# Ensure required columns are present in the existing inscope_vinfo_df\n",
    "required_columns = ['Environment', 'Final OS', 'Disk Size TB', 'VM', 'Cluster', 'Disk Classification']\n",
    "missing_columns = [col for col in required_columns if col not in inscope_vinfo_df.columns]\n",
    "\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"The following required columns are missing in inscope_vinfo_df: {missing_columns}\")\n",
    "\n",
    "# Preprocess columns\n",
    "inscope_vinfo_df['Final OS'] = inscope_vinfo_df['Final OS'].fillna(\"Unknown OS\").astype(str).str.strip()\n",
    "inscope_vinfo_df['Environment'] = inscope_vinfo_df['Environment'].fillna('unknown').str.lower().str.strip()\n",
    "inscope_vinfo_df['Disk Size TB'] = pd.to_numeric(inscope_vinfo_df['Disk Size TB'], errors='coerce')\n",
    "inscope_vinfo_df['VM'] = inscope_vinfo_df['VM'].fillna('Unknown VM')\n",
    "inscope_vinfo_df['Cluster'] = inscope_vinfo_df['Cluster'].fillna('').str.lower()\n",
    "\n",
    "# Debug: Check unique OS values\n",
    "print(\"Unique Final OS values:\", inscope_vinfo_df['Final OS'].unique())\n",
    "\n",
    "# Derive F5-Hard complexity directly from Disk Classification\n",
    "inscope_vinfo_df['Complexity'] = inscope_vinfo_df['Disk Classification'].apply(\n",
    "    lambda x: 'F5-Hard' if 'F5-Hard' in str(x) else None\n",
    ")\n",
    "\n",
    "# Assign complexities based on OS and Disk Size\n",
    "def classify_complexity(row):\n",
    "    if row['Complexity'] == 'F5-Hard':  # Skip if already classified as F5-Hard\n",
    "        return 'Hard'  # Combine F5-Hard into Hard\n",
    "\n",
    "    if 'oracle' in row['Final OS'].lower():\n",
    "        return 'Oracle-DBs'\n",
    "    if any(keyword in row['Final OS'].lower() for keyword in ['mssql', 'sql server', 'microsoft sql', 'ms sql', 'sql database', 'sql']):\n",
    "        return 'MSSQL-DBs'\n",
    "\n",
    "    if row['Disk Size TB'] <= 2:\n",
    "        return 'Easy'\n",
    "    elif row['Disk Size TB'] <= 5:\n",
    "        return 'Medium'\n",
    "    elif row['Disk Size TB'] <= 10:\n",
    "        return 'Hard'\n",
    "    else:\n",
    "        return 'Hard'\n",
    "\n",
    "inscope_vinfo_df['Complexity'] = inscope_vinfo_df.apply(classify_complexity, axis=1)\n",
    "\n",
    "# Define the custom sorting order for Complexity\n",
    "complexity_order = ['Easy', 'Medium', 'Hard', 'Oracle-DBs', 'MSSQL-DBs']\n",
    "inscope_vinfo_df['Complexity'] = pd.Categorical(\n",
    "    inscope_vinfo_df['Complexity'],\n",
    "    categories=complexity_order,\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "# Sort the DataFrame by Complexity\n",
    "inscope_vinfo_df = inscope_vinfo_df.sort_values('Complexity')\n",
    "\n",
    "# Create the OS Support column based on supported_os_list\n",
    "inscope_vinfo_df['OS Support'] = inscope_vinfo_df['Final OS'].apply(\n",
    "    lambda os: 'Supported' if any(supported_os in os for supported_os in supported_os_list) else 'Not Supported'\n",
    ")\n",
    "\n",
    "# Add Migration Time\n",
    "inscope_vinfo_df['Migration Time (minutes)'] = inscope_vinfo_df['Disk Size TB'].apply(\n",
    "    lambda size: ((size * 1024) / 500) * migration_time_per_500gb\n",
    ")\n",
    "\n",
    "# Add Post-Migration Troubleshooting Time\n",
    "pmt_minutes = pmt_hours * 60\n",
    "inscope_vinfo_df['Total Time (minutes)'] = inscope_vinfo_df['Migration Time (minutes)'] + pmt_minutes\n",
    "\n",
    "# Summarize by Complexity and OS Support\n",
    "disk_classification_summary = inscope_vinfo_df.groupby(['Complexity', 'OS Support'], observed=True).agg(\n",
    "    VM_Count=('VM', 'count'),\n",
    "    Total_Disk=('Disk Size TB', 'sum'),\n",
    "    Total_Mig_Time_Minutes=('Total Time (minutes)', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Include all \"OS Support\" categories in the summary\n",
    "for complexity in complexity_order:\n",
    "    for os_support in ['Supported', 'Not Supported']:\n",
    "        if not ((disk_classification_summary['Complexity'] == complexity) &\n",
    "                (disk_classification_summary['OS Support'] == os_support)).any():\n",
    "            disk_classification_summary = pd.concat([\n",
    "                disk_classification_summary,\n",
    "                pd.DataFrame({\n",
    "                    'Complexity': [complexity],\n",
    "                    'OS Support': [os_support],\n",
    "                    'VM_Count': [0],\n",
    "                    'Total_Disk': [0],\n",
    "                    'Total_Mig_Time_Minutes': [0]\n",
    "                })\n",
    "            ], ignore_index=True)\n",
    "\n",
    "# Filter out rows where VM_Count is 0\n",
    "disk_classification_summary = disk_classification_summary[disk_classification_summary['VM_Count'] > 0]\n",
    "\n",
    "# Add calculated columns for formatted migration time and days\n",
    "disk_classification_summary['Formatted_Mig_Time'] = disk_classification_summary['Total_Mig_Time_Minutes'].apply(\n",
    "    lambda minutes: f\"{math.ceil(minutes / 30) / 2:,.1f}h\"\n",
    ")\n",
    "disk_classification_summary['Days_Per_FTEs'] = disk_classification_summary['Total_Mig_Time_Minutes'].apply(\n",
    "    lambda minutes: math.ceil(minutes / (fte_hours_per_day * 60 * fte_count))\n",
    ")\n",
    "\n",
    "# Format VM Count with thousands separator\n",
    "disk_classification_summary['VM_Count'] = disk_classification_summary['VM_Count'].apply(lambda x: f\"{x:,}\")\n",
    "\n",
    "# Format Total Disk (TB) without rounding\n",
    "disk_classification_summary['Total_Disk'] = disk_classification_summary['Total_Disk'].apply(lambda x: f\"{x:,.2f}\")\n",
    "\n",
    "# Add totals row\n",
    "totals_row = {\n",
    "    'Complexity': 'Totals',\n",
    "    'OS Support': '',\n",
    "    'VM_Count': f\"{int(disk_classification_summary['VM_Count'].str.replace(',', '').astype(int).sum()):,}\",\n",
    "    'Total_Disk': f\"{float(disk_classification_summary['Total_Disk'].str.replace(',', '').astype(float).sum()):,.2f}\",\n",
    "    'Formatted_Mig_Time': f\"{math.ceil(disk_classification_summary['Total_Mig_Time_Minutes'].sum() / 30) / 2:,.1f}h\",\n",
    "    'Days_Per_FTEs': disk_classification_summary['Days_Per_FTEs'].sum()\n",
    "}\n",
    "disk_classification_summary = pd.concat(\n",
    "    [disk_classification_summary, pd.DataFrame([totals_row])],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# Print the table\n",
    "headers = [\n",
    "    \"Complexity\", \"OS Support\", \"VM Count\", \"Total Disk (TB)\",\n",
    "    \"Total Migration Time\", f\"Days ({fte_count} FTEs)\"\n",
    "]\n",
    "rows = disk_classification_summary[\n",
    "    ['Complexity', 'OS Support', 'VM_Count', 'Total_Disk', 'Formatted_Mig_Time', 'Days_Per_FTEs']\n",
    "].values.tolist()\n",
    "\n",
    "print(\"\\nSummary Table:\")\n",
    "print(custom_table_format_with_totals(headers, rows))\n",
    "\n",
    "# Display Oracle-DBs and MSSQL-DBs separately\n",
    "oracle_summary = disk_classification_summary[disk_classification_summary['Complexity'] == 'Oracle-DBs']\n",
    "mssql_summary = disk_classification_summary[disk_classification_summary['Complexity'] == 'MSSQL-DBs']\n",
    "\n",
    "print(\"\\nOracle-DBs Summary:\")\n",
    "print(oracle_summary)\n",
    "\n",
    "print(\"\\nMSSQL-DBs Summary:\")\n",
    "print(mssql_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff9c5ba-e008-4288-9fe6-0bfd258fe046",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
