{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaa7e99f-0c30-40e7-be7c-37f29aa9007f",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This Python 3 notebook explores the exported VMWare inventory generated by [RVTools](https://www.robware.net/) and generates various statistical analyses of the data for assessing the scale, complexity, and other characteristics.\n",
    "\n",
    "The analysis is directed towards understanding the feasibility of migrating these VMWare virtual machines to Red Hat's OpenShift Virtualization Platform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e010991-ca96-43de-af5f-cd22bfdbd72e",
   "metadata": {},
   "source": [
    "# Initial Setup\n",
    "\n",
    "This section configures the script and the required packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Required Packages\n",
    "\n",
    "This script uses `pandas` and `numpy` for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the VM inventory generated by rvtools\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "Set up a few options and other configuration parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the display option to prevent line wrapping\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.expand_frame_repr', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Set up the data directory\n",
    "First, set up the directory where the RVTools Excel data files will be stored. This folder must also contain an index.xlsx file (refer to the provided template for the expected format). The index.xlsx file should list the valid RVTools Excel file names along with their corresponding vCenter instances. This script assumes that there is one Excel file per vCenter instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current script directory\n",
    "current_dir = \".\"\n",
    "\n",
    "# Specify the directory containing the Excel files\n",
    "DATA_DIR = os.path.join(current_dir, '../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and configuration parameters\n",
    "\n",
    "# The index file\n",
    "INDEX_FILENAME = \"index.xlsx\"\n",
    "INDEX_FILEPATH = os.path.join(DATA_DIR, INDEX_FILENAME)\n",
    "INDEX_SHEETNAME = \"index\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Functions\n",
    "\n",
    "This section defines a few functions for decomposing the analysis code. It reads multiple Excel .xlsx files exported from RVtools software and returns a dictionary with filenames as keys and a dictionary of DataFrames (one for each sheet) as values.\n",
    "\n",
    "* The `index.xlsx` and `index_template.xlsx` files are explicitly ignored.\n",
    "\n",
    "Parameters:\n",
    "* directory (str): The directory containing the Excel files.\n",
    "* filenames_to_process (list): A list of filenames to process.\n",
    "\n",
    "Returns:\n",
    "* dict: A dictionary containing the DataFrames from each Excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_rvtools_excel_files(directory, filenames_to_process):\n",
    "    rvtools_data = {}\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        # Only process files listed in the index file.\n",
    "        filename_base, _ = os.path.splitext(filename)\n",
    "        if filename_base not in filenames_to_process: continue\n",
    "        if filename in ['index.xlsx', 'index_template.xlsx']: continue\n",
    "\n",
    "        if filename.endswith('.xlsx') or filename.endswith('.xls'):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            excel_data = pd.read_excel(filepath, sheet_name=None)\n",
    "            rvtools_data[filename] = excel_data\n",
    "\n",
    "    return rvtools_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "# Read, Clean and Filter the RVTools data\n",
    "\n",
    "This section reads the `RVTools` exported files. It uses an `index.xlsx` metadata file to identify the\n",
    "in-scope `vCenter` instances for the migration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Read the index metadata file\n",
    "\n",
    "First read the index file to determine the RVTools files to process. This file contains additional metadata,\n",
    "including the `vCenter` instances In-Scope, which can be customized to process specific instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First read the index Excel file\n",
    "import pandas as pd\n",
    "\n",
    "# Read the index Excel file\n",
    "index_df = pd.read_excel(\n",
    "    INDEX_FILEPATH, sheet_name=INDEX_SHEETNAME, \n",
    "    nrows=19, index_col='vCenter', \n",
    "    true_values=['Yes', 'Y'], false_values=['No', 'N'], \n",
    "    na_filter=False, dtype={'In Scope': bool}\n",
    ")\n",
    "\n",
    "# Clean up column names and handle missing values\n",
    "index_df.columns = index_df.columns.str.replace(' ', '_')\n",
    "index_df.fillna('', inplace=True)\n",
    "\n",
    "# Filter for in-scope vCenters\n",
    "inscope_df = index_df[index_df['In_Scope']].reset_index()\n",
    "\n",
    "# Count occurrences of each vCenter\n",
    "pivot_table = inscope_df.groupby('vCenter').size().reset_index(name='Count')\n",
    "\n",
    "# Print in a structured table format\n",
    "print(\"\\nIn-Scope vCenter Instances:\")\n",
    "print(pivot_table.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00709fbf-13ea-49e8-af60-8fac524d0244",
   "metadata": {},
   "source": [
    "### Read the RVTool Exported Spreadsheets\n",
    "\n",
    "Read all the RVTools exported files available in the `data` directory. The data will be read into a dictionary,\n",
    "with one entry per file, where the key is the filename, and the value is **another** nested dictionary with the spreadsheet's\n",
    "sheet name as the key, and a `DataFrame` containing the sheets values.\n",
    "\n",
    "The files **need** to be in the Microsoft `xlsx` format.\n",
    "\n",
    "**Note**: This step will take a few minutes to complete. Be patient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fce93b-3805-4903-becf-43b2fc78bf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Read the index Excel file\n",
    "index_df = pd.read_excel(\n",
    "    INDEX_FILEPATH, sheet_name=INDEX_SHEETNAME, \n",
    "    nrows=19, index_col='vCenter', \n",
    "    true_values=['Yes', 'Y'], false_values=['No', 'N'], \n",
    "    na_filter=False, dtype={'In Scope': bool}\n",
    ")\n",
    "\n",
    "# Clean up column names and handle missing values\n",
    "index_df.columns = index_df.columns.str.replace(' ', '_')\n",
    "index_df.fillna('', inplace=True)\n",
    "\n",
    "# Ensure 'In_Scope' column is boolean\n",
    "index_df['In_Scope'] = index_df['In_Scope'].astype(bool)\n",
    "\n",
    "# Filter for in-scope vCenters\n",
    "inscope_df = index_df[index_df['In_Scope']].reset_index()\n",
    "\n",
    "# Extract list of in-scope vCenter instances\n",
    "inscope_vcenter_instances = inscope_df['vCenter'].tolist()\n",
    "\n",
    "# Function to read RVTools Excel files while excluding 'index_template.xlsx' and 'index.xlsx'\n",
    "def read_rvtools_excel_files(directory, vcenters):\n",
    "    rvtools_data = {}\n",
    "    exclude_files = {\"index_template.xlsx\", \"index.xlsx\"}  # Set of filenames to exclude\n",
    "\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".xlsx\") and file.lower() not in exclude_files:\n",
    "            file_path = os.path.join(directory, file)\n",
    "            try:\n",
    "                xls = pd.ExcelFile(file_path)\n",
    "                sheets = {sheet: xls.parse(sheet) for sheet in xls.sheet_names}\n",
    "                rvtools_data[file] = sheets\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file}: {e}\")\n",
    "    \n",
    "    return rvtools_data\n",
    "\n",
    "# Read the RVTools Excel files\n",
    "rvtools_data = read_rvtools_excel_files(\"../data\", inscope_vcenter_instances)\n",
    "\n",
    "# Display the loaded data (for demonstration purposes)\n",
    "for filename, sheets in rvtools_data.items():\n",
    "    print(f'Processed RVTools File: {filename}')\n",
    "    for sheet_name, df in sheets.items():\n",
    "        print(f\"  Sheet: {sheet_name} Shape: {df.shape}\")\n",
    "\n",
    "print(f'Total files processed: {len(rvtools_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Create the consolidated data frames\n",
    "\n",
    "Create two consolidated dataframes containing information from all `vCenter` instances:\n",
    "\n",
    "1. One dataframe containing the `vInfo` details\n",
    "2. The second one containing the `vHost` details\n",
    "\n",
    "These will be used later to summarize the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize the lists\n",
    "vinfo_sheets = []\n",
    "vhost_sheets = []\n",
    "\n",
    "# Load vInfo and vHost sheets into the lists\n",
    "for filename, sheets in rvtools_data.items():\n",
    "    vCenter = filename.split('.')[0].lower()  # Extract vCenter instance name from the filename\n",
    "\n",
    "    # Ensure 'vInfo' and 'vHost' sheets exist before processing\n",
    "    if 'vInfo' in sheets and 'vHost' in sheets:\n",
    "        # Add the vCenter instance name to each sheet\n",
    "        sheets['vInfo']['vCenter'] = vCenter\n",
    "        sheets['vHost']['vCenter'] = vCenter\n",
    "\n",
    "        # Append to the respective lists\n",
    "        vinfo_sheets.append(sheets['vInfo'])\n",
    "        vhost_sheets.append(sheets['vHost'])\n",
    "\n",
    "# Filter out empty or all-NA DataFrames\n",
    "vinfo_sheets = [df for df in vinfo_sheets if not df.dropna(how='all').empty]\n",
    "vhost_sheets = [df for df in vhost_sheets if not df.dropna(how='all').empty]\n",
    "\n",
    "# Concatenate the filtered DataFrames only if there are valid sheets\n",
    "consolidated_vinfo_df = pd.concat(vinfo_sheets, ignore_index=True) if vinfo_sheets else pd.DataFrame()\n",
    "consolidated_vhost_df = pd.concat(vhost_sheets, ignore_index=True) if vhost_sheets else pd.DataFrame()\n",
    "\n",
    "# Verify data ingestion success with improved readability\n",
    "print(\"\\nâœ… Data Ingestion Verification âœ…\")\n",
    "print(f\"Total vInfo (Total VM's) records: {len(consolidated_vinfo_df)}\")\n",
    "print(f\"Total vHost (Total HW) records: {len(consolidated_vhost_df)}\\n\")\n",
    "\n",
    "if not consolidated_vinfo_df.empty:\n",
    "    print(\"\\nðŸ”¹ Sample vInfo Data:\")\n",
    "    print(consolidated_vinfo_df.head(2).to_string(index=False), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726fe759-4222-4227-a0e1-896b7e60de1b",
   "metadata": {},
   "source": [
    "### Distribution of VMs per In-Scope vCenter\n",
    "\n",
    "What is the total percentage of VM's per vCenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Group by vCenter and count VMs\n",
    "grouped_summary = consolidated_vinfo_df.groupby(\"vCenter\").size().reset_index(name=\"Count\")\n",
    "\n",
    "# Create a pivot table for better visualization\n",
    "grouped_summary_pivot = grouped_summary.pivot_table(values=\"Count\", index=\"vCenter\", aggfunc=\"sum\")\n",
    "\n",
    "# Print summary\n",
    "total_vms = grouped_summary[\"Count\"].sum()\n",
    "total_vcenters = grouped_summary.shape[0]\n",
    "\n",
    "print(f\"Overall Distribution of {total_vms:,} VMs in the {total_vcenters:,} vCenter instances:\\n\")\n",
    "print(grouped_summary_pivot)\n",
    "print(\"\\nNote: This is the TOTAL VM count, and will include VM templates, SRM placeholders, Orphaned objects...\")\n",
    "\n",
    "# Plot a pie chart\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(\n",
    "    grouped_summary[\"Count\"],\n",
    "    labels=grouped_summary[\"vCenter\"],\n",
    "    autopct=\"%1.1f%%\",\n",
    "    startangle=140,\n",
    ")\n",
    "plt.title(\"\\nDistribution of VMs by vCenter Instances\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Clean the VM List\n",
    "Filter the consolidated `vInfo` content using the following criteria:\n",
    "\n",
    "1. Remove all `template` entries\n",
    "2. Remove all `SRM Placeholder` entries\n",
    "3. Remove all `orphaned VM object` entries\n",
    "4. Remove all `other objects`\n",
    "\n",
    "Adjust `ignored_patterns` as well as `os_filter_patterns` to suit your needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61863a1-bd86-445c-b261-d22d12c13cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "# Define patterns to ignore in the VM column\n",
    "ignore_patterns = [\"virtual_appliance\", \"virtual appliance\", \"CTX\"]\n",
    "\n",
    "# Define OS types to filter out (only using \"OS according to the VMware Tools\")\n",
    "os_filter_patterns = [\n",
    "    r\"^Microsoft Windows 7(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Microsoft Windows Server 2003 Standard(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Microsoft Windows Server 2003(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Other$\",\n",
    "    r\"^Other\\s+\\d+\\.\\d+.*$\",\n",
    "    r\"^Other 3\\.x or later Linux(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Other 3\\.x Linux(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Other 4\\.x or later Linux(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Other 5\\.x or later Linux(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^VMware Photon OS(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Microsoft Windows Server 2008(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Microsoft Windows Server 2008 R2(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Red Hat Enterprise Linux (4|5|6)(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Debian GNU/Linux (7|8|9|10|11)(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^CentOS (4|5|6|7)(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^CentOS 4/5/6/7(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^CentOS 4/5(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^CentOS 4/5/6(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^CentOS Stream 8(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Ubuntu Linux(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Linux \\d+\\.\\d+.*$\",\n",
    "    r\"^Appgate(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Amazon Linux 2(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Other Linux(?:\\s+.*)?$\",\n",
    "    r\"^FreeBSD(?:\\s+.*)?$\",\n",
    "    r\"^RiOS(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Red Hat Fedora(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^SuSE Linux Enterprise (11|12)(?:\\s*\\(.*\\))?$\"\n",
    "]\n",
    "\n",
    "def clean_os_name(os_name):\n",
    "    \"\"\"Normalize OS names by removing extra spaces and redundant information.\"\"\"\n",
    "    if pd.isna(os_name) or os_name.strip() == '':\n",
    "        return ''\n",
    "    os_name = os_name.strip()\n",
    "    os_name = re.sub(r\"\\s*\\(.*\\)$\", \"\", os_name)  # Remove (32-bit) / (64-bit)\n",
    "    os_name = re.sub(r\"\\s+\", \" \", os_name)  # Remove extra spaces\n",
    "    return os_name\n",
    "\n",
    "def os_filter(os_name):\n",
    "    \"\"\"Check if the OS should be filtered based on the defined patterns.\"\"\"\n",
    "    if pd.isna(os_name) or os_name.strip() == '':\n",
    "        return False\n",
    "    os_name_clean = clean_os_name(os_name)\n",
    "    return any(re.fullmatch(pattern, os_name_clean, re.IGNORECASE) for pattern in os_filter_patterns)\n",
    "\n",
    "# Load or define consolidated_vinfo_df before processing\n",
    "if 'consolidated_vinfo_df' not in globals():\n",
    "    raise ValueError(\"consolidated_vinfo_df is not defined.\")\n",
    "\n",
    "# Ensure OS column is populated\n",
    "consolidated_vinfo_df['OS Effective'] = consolidated_vinfo_df['OS according to the VMware Tools'].fillna(\n",
    "    consolidated_vinfo_df['OS according to the configuration file'])\n",
    "\n",
    "# Normalize OS names before filtering\n",
    "consolidated_vinfo_df['Cleaned OS'] = consolidated_vinfo_df['OS Effective'].apply(clean_os_name)\n",
    "\n",
    "# Apply filtering logic\n",
    "consolidated_vinfo_df['Exclusion Reason'] = ''\n",
    "consolidated_vinfo_df.loc[consolidated_vinfo_df['VM'].str.contains('|'.join(ignore_patterns), case=False, na=False), 'Exclusion Reason'] = 'Ignored VM Pattern'\n",
    "consolidated_vinfo_df.loc[consolidated_vinfo_df['Cleaned OS'].apply(os_filter), 'Exclusion Reason'] = 'Excluded OS'\n",
    "consolidated_vinfo_df.loc[consolidated_vinfo_df['Template'].fillna(False) == True, 'Exclusion Reason'] = 'Template'\n",
    "consolidated_vinfo_df.loc[consolidated_vinfo_df['SRM Placeholder'].fillna(False) == True, 'Exclusion Reason'] = 'SRM Placeholder'\n",
    "consolidated_vinfo_df.loc[consolidated_vinfo_df['Connection state'].fillna('').str.lower() == 'orphaned', 'Exclusion Reason'] = 'Orphaned VM'\n",
    "\n",
    "# Only exclude powered-off VMs if their OS is in the exclusion list\n",
    "consolidated_vinfo_df.loc[(consolidated_vinfo_df['Powerstate'].fillna('').str.lower() == 'poweredoff') & \n",
    "                          (consolidated_vinfo_df['Cleaned OS'].apply(os_filter)), 'Exclusion Reason'] = 'Powered Off'\n",
    "\n",
    "# Filter only in-scope VMs\n",
    "filtered_vinfo_df = consolidated_vinfo_df[consolidated_vinfo_df['Exclusion Reason'] == '']\n",
    "\n",
    "# Count the number of VMs that were filtered out\n",
    "ignored_vinfo_df = consolidated_vinfo_df[consolidated_vinfo_df['Exclusion Reason'] != '']\n",
    "ignored_vm_artifacts = len(ignored_vinfo_df)  # Define ignored_vm_artifacts\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\\U0001F50D Filtering Summary\")\n",
    "print(f\"\\U0001F539 Removed: {ignored_vm_artifacts:,} VM templates, SRM placeholders, orphaned, powered-off VMs, and excluded OS patterns.\")\n",
    "print(f\"\\U0001F539\\U0001F539 Filtered (In-Scope) VM count: {len(filtered_vinfo_df):,}.\\n\")\n",
    "\n",
    "# Display the list of \"in-scope\" VMs after filtering\n",
    "if not filtered_vinfo_df.empty:\n",
    "    print(\"\\nâœ… Sample of filtered (In-Scope) VMs:\")\n",
    "    display(filtered_vinfo_df[['VM', 'OS Effective']].head())\n",
    "\n",
    "# Display the list of \"ignored\" VMs\n",
    "if not ignored_vinfo_df.empty:\n",
    "    print(\"\\nâŒ Sample of filtered (Out-of-Scope) VMs:\")\n",
    "    display(ignored_vinfo_df[['VM', 'OS Effective', 'Exclusion Reason']].head())\n",
    "\n",
    "# Create pie chart\n",
    "labels = ['Filtered-Out', 'In-Scope']\n",
    "sizes = [ignored_vm_artifacts, len(filtered_vinfo_df)]\n",
    "colors = ['lightcoral', 'lightgreen']\n",
    "explode = (0.1, 0)  # explode the 'Filtered Out' section for emphasis\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.pie(sizes, labels=labels, autopct='%1.1f%%', colors=colors, explode=explode, shadow=True, startangle=140)\n",
    "plt.title(\"\\nVM Filtering Breakdown\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Summary of vInfo & vHosts after cleanup\n",
    "\n",
    "Now we have clean data to play with going forward..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Select the in-scope vCenter instances\n",
    "inscope_vinfo_condition = filtered_vinfo_df['vCenter'].isin(inscope_vcenter_instances)\n",
    "inscope_vinfo_df = filtered_vinfo_df[inscope_vinfo_condition]\n",
    "\n",
    "# Summary stats for in-scope VMs\n",
    "inscope_vm_count = len(filtered_vinfo_df)\n",
    "percent_inscope_vms = (inscope_vm_count / len(filtered_vinfo_df)) * 100.0\n",
    "\n",
    "print(\"\\nâœ… VM Summary âœ…\")\n",
    "print(f\"ðŸ”¹ {inscope_vm_count:,} VMs are in-scope ({percent_inscope_vms:0.2f}% of total VMs).\\n\")\n",
    "\n",
    "# Create a pivot table for VMs by vCenter\n",
    "vm_pivot = inscope_vinfo_df.pivot_table(index=\"vCenter\", aggfunc=\"size\").reset_index()\n",
    "vm_pivot.columns = [\"vCenter\", \"VM Count\"]\n",
    "display(vm_pivot)\n",
    "\n",
    "# Select the in-scope hosts\n",
    "inscope_vhost_condition = consolidated_vhost_df['vCenter'].isin(inscope_vcenter_instances)\n",
    "inscope_vhost_df = consolidated_vhost_df[inscope_vhost_condition]\n",
    "\n",
    "# Summary stats for in-scope hosts\n",
    "inscope_host_count = len(inscope_vhost_df)\n",
    "percent_inscope_hosts = (inscope_host_count / len(consolidated_vhost_df)) * 100.0\n",
    "\n",
    "print(\"\\nâœ… Host Summary âœ…\")\n",
    "print(f\"ðŸ”¹ {inscope_host_count:,} hosts are in-scope ({percent_inscope_hosts:0.2f}% of total hosts).\\n\")\n",
    "\n",
    "# Create a pivot table for hosts by vCenter\n",
    "host_pivot = inscope_vhost_df.pivot_table(index=\"vCenter\", aggfunc=\"size\").reset_index()\n",
    "host_pivot.columns = [\"vCenter\", \"Host Count\"]\n",
    "display(host_pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabb6031-4de5-4f22-937a-ece58b28b701",
   "metadata": {},
   "source": [
    "# ========== Analysis =========="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "The primary analysis begins from this section forward..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### 1- Create a consolidated view of the client's landscape\n",
    "\n",
    "Summary of VM's (vCPU, Memory, etc) & Host's (CPU, Cores, etc)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === vInfo Pivot Table (VM-Level Info) ===\n",
    "vinfo_pivot_df = filtered_vinfo_df.pivot_table(\n",
    "    index='vCenter',\n",
    "    values=['VM', 'CPUs', 'Memory', 'NICs', 'Provisioned MiB'],\n",
    "    aggfunc={\n",
    "        'VM': 'count',\n",
    "        'CPUs': 'sum',\n",
    "        'Memory': 'sum',\n",
    "        'NICs': 'sum',\n",
    "        'Provisioned MiB': 'sum'\n",
    "    },\n",
    "    margins=False\n",
    ")\n",
    "\n",
    "# === vHost Pivot Table (Host-Level Info) ===\n",
    "vhost_pivot_df = consolidated_vhost_df.pivot_table(\n",
    "    index='vCenter',\n",
    "    values=['Host', '# VMs total', '# CPU', '# Cores'],\n",
    "    aggfunc={\n",
    "        'Host': 'count',\n",
    "        '# VMs total': 'sum',\n",
    "        '# CPU': 'sum',\n",
    "        '# Cores': 'sum'\n",
    "    },\n",
    "    margins=False\n",
    ")\n",
    "\n",
    "# === Convert Memory & Disk to GB/TB ===\n",
    "def format_storage(mib_value):\n",
    "    if mib_value >= 1_000_000:  # Convert to TB if â‰¥ 1,000,000 MiB\n",
    "        return f\"{mib_value / 1_048_576:.2f} TB\"\n",
    "    return f\"{mib_value / 1024:.2f} GB\"  # Convert to GB otherwise\n",
    "\n",
    "# Apply formatting for Memory and Total Disk Capacity\n",
    "vinfo_pivot_df['Total Disk Capacity'] = vinfo_pivot_df['Provisioned MiB'].apply(format_storage)\n",
    "vinfo_pivot_df['Memory'] = vinfo_pivot_df['Memory'].apply(format_storage)\n",
    "\n",
    "# === Pretty Display for Each Table ===\n",
    "try:\n",
    "    from IPython.display import display  # Works for Jupyter Notebook\n",
    "\n",
    "    print(\"\\nâœ… VM Info (vInfo) âœ…\")\n",
    "    display(vinfo_pivot_df)\n",
    "\n",
    "    print(\"\\nâœ… Host Info (vHost) âœ…\")\n",
    "    display(vhost_pivot_df)\n",
    "\n",
    "except ImportError:\n",
    "    print(\"\\nâœ… VM Info (vInfo) âœ…\")\n",
    "    print(vinfo_pivot_df)\n",
    "\n",
    "    print(\"\\nâœ… Host Info (vHost) âœ…\")\n",
    "    print(vhost_pivot_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### 2- Summarize the Operating Systems\n",
    "\n",
    "In this section, we summarize the guest operating systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "# Define patterns to ignore in the VM column\n",
    "ignore_patterns = [\"virtual_appliance\", \"virtual appliance\", \"CTX\"]\n",
    "\n",
    "# Define OS types to filter out (only using \"OS according to the VMware Tools\")\n",
    "os_filter_patterns = [\n",
    "    r\"^Microsoft Windows 7(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Microsoft Windows Server 2003 Standard(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Microsoft Windows Server 2003(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Other$\",\n",
    "    r\"^Other\\s+\\d+\\.\\d+.*$\",\n",
    "    r\"^Other 3\\.x or later Linux(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Other 3\\.x Linux(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Other 4\\.x or later Linux(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Other 5\\.x or later Linux(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^VMware Photon OS(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Microsoft Windows Server 2008(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Microsoft Windows Server 2008 R2(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Red Hat Enterprise Linux (4|5|6)(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Debian GNU/Linux (7|8|9|10|11)(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^CentOS (4|5|6|7)(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^CentOS 4/5/6/7(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^CentOS 4/5(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^CentOS 4/5/6(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^CentOS Stream 8(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Ubuntu Linux(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Linux \\d+\\.\\d+.*$\",\n",
    "    r\"^Appgate(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Amazon Linux 2(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Other Linux(?:\\s+.*)?$\",\n",
    "    r\"^FreeBSD(?:\\s+.*)?$\",\n",
    "    r\"^RiOS(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^Red Hat Fedora(?:\\s*\\(.*\\))?$\",\n",
    "    r\"^SuSE Linux Enterprise (11|12)(?:\\s*\\(.*\\))?$\"\n",
    "]\n",
    "\n",
    "def clean_os_name(os_name):\n",
    "    \"\"\"Normalize OS names by removing extra spaces and redundant information.\"\"\"\n",
    "    if pd.isna(os_name) or os_name.strip() == '':\n",
    "        return ''\n",
    "    os_name = os_name.strip()\n",
    "    os_name = re.sub(r\"\\s*\\(.*\\)$\", \"\", os_name)  # Remove (32-bit) / (64-bit)\n",
    "    os_name = re.sub(r\"\\s+\", \" \", os_name)  # Remove extra spaces\n",
    "    return os_name\n",
    "\n",
    "def os_filter(os_name):\n",
    "    \"\"\"Check if the OS should be filtered based on the defined patterns.\"\"\"\n",
    "    if pd.isna(os_name) or os_name.strip() == '':\n",
    "        return False\n",
    "    os_name_clean = clean_os_name(os_name)\n",
    "    return any(re.fullmatch(pattern, os_name_clean, re.IGNORECASE) for pattern in os_filter_patterns)\n",
    "\n",
    "# Load or define consolidated_vinfo_df before processing\n",
    "if 'consolidated_vinfo_df' not in globals():\n",
    "    raise ValueError(\"consolidated_vinfo_df is not defined.\")\n",
    "\n",
    "# Ensure OS column is populated\n",
    "consolidated_vinfo_df['OS Effective'] = consolidated_vinfo_df['OS according to the VMware Tools'].fillna(\n",
    "    consolidated_vinfo_df['OS according to the configuration file'])\n",
    "\n",
    "# Normalize OS names before filtering\n",
    "consolidated_vinfo_df['Cleaned OS'] = consolidated_vinfo_df['OS Effective'].apply(clean_os_name)\n",
    "\n",
    "# Apply filtering logic\n",
    "consolidated_vinfo_df['Exclusion Reason'] = ''\n",
    "consolidated_vinfo_df.loc[consolidated_vinfo_df['VM'].str.contains('|'.join(ignore_patterns), case=False, na=False), 'Exclusion Reason'] = 'Ignored VM Pattern'\n",
    "consolidated_vinfo_df.loc[consolidated_vinfo_df['Cleaned OS'].apply(os_filter), 'Exclusion Reason'] = 'Excluded OS'\n",
    "consolidated_vinfo_df.loc[consolidated_vinfo_df['Template'].fillna(False) == True, 'Exclusion Reason'] = 'Template'\n",
    "consolidated_vinfo_df.loc[consolidated_vinfo_df['SRM Placeholder'].fillna(False) == True, 'Exclusion Reason'] = 'SRM Placeholder'\n",
    "consolidated_vinfo_df.loc[consolidated_vinfo_df['Connection state'].fillna('').str.lower() == 'orphaned', 'Exclusion Reason'] = 'Orphaned VM'\n",
    "\n",
    "# Only exclude powered-off VMs if their OS is in the exclusion list\n",
    "consolidated_vinfo_df.loc[(consolidated_vinfo_df['Powerstate'].fillna('').str.lower() == 'poweredoff') & \n",
    "                          (consolidated_vinfo_df['Cleaned OS'].apply(os_filter)), 'Exclusion Reason'] = 'Powered Off'\n",
    "\n",
    "# Filter only in-scope VMs\n",
    "filtered_vinfo_df = consolidated_vinfo_df[consolidated_vinfo_df['Exclusion Reason'] == '']\n",
    "\n",
    "# Create the pivot table\n",
    "inscope_guest_os_pivot = filtered_vinfo_df.pivot_table(index='Cleaned OS', values='VM', aggfunc='count')\n",
    "inscope_guest_os_pivot = inscope_guest_os_pivot.sort_values(by='VM', ascending=True)\n",
    "\n",
    "# Check if pivot table is empty\n",
    "if inscope_guest_os_pivot.empty:\n",
    "    raise ValueError(\"The pivot table is empty after filtering. Check input data.\")\n",
    "\n",
    "# Calculate percentages\n",
    "total_vms = inscope_guest_os_pivot['VM'].sum()\n",
    "percentages = (inscope_guest_os_pivot['VM'] / total_vms * 100).round(1)\n",
    "\n",
    "# Create figure and axis with better layout management\n",
    "fig, ax = plt.subplots(figsize=(14, max(6, len(inscope_guest_os_pivot) * 0.4)), constrained_layout=True)\n",
    "\n",
    "# Create horizontal bar chart\n",
    "bars = ax.barh(inscope_guest_os_pivot.index, inscope_guest_os_pivot['VM'], color=plt.cm.tab10.colors)\n",
    "\n",
    "# Add labels to bars\n",
    "for bar, count, percentage in zip(bars, inscope_guest_os_pivot['VM'], percentages):\n",
    "    ax.text(bar.get_width() + 2, bar.get_y() + bar.get_height()/2, \n",
    "            f\"{count} VMs ({percentage}%)\", va='center', fontsize=8, color='black')\n",
    "\n",
    "# Style and labels\n",
    "ax.set_xlabel(\"Number of VMs\", fontsize=10)\n",
    "ax.set_ylabel(\"Operating Systems\", fontsize=10)\n",
    "ax.set_title(\"Guest OS Distribution for In-Scope VMs\", fontsize=12, pad=20)\n",
    "ax.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Explicitly set y-ticks before modifying labels to avoid warnings\n",
    "ax.set_yticks(range(len(inscope_guest_os_pivot)))\n",
    "ax.set_yticklabels(inscope_guest_os_pivot.index, fontsize=9, rotation=5)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### 3- OSs with over 500 VMs Associated\n",
    "\n",
    "In this section, we summarize the guest operating systems that have more than 500 VM's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Filter and display OSs with over 500 VMs\n",
    "over_500_vms = inscope_guest_os_pivot[inscope_guest_os_pivot['VM'] > 500]\n",
    "display(over_500_vms)\n",
    "\n",
    "# Check if there are any OSs with more than 500 VMs\n",
    "if over_500_vms.empty:\n",
    "    print(\"No operating systems have more than 500 VMs.\")\n",
    "else:\n",
    "    # Pie chart for OSs with over 500 VMs\n",
    "    plt.figure(figsize=(10, 8))  # Slightly wider for better readability\n",
    "\n",
    "    # Define a color palette\n",
    "    colors = plt.cm.tab10.colors[:len(over_500_vms)]\n",
    "\n",
    "    # Generate pie chart\n",
    "    wedges, texts, autotexts = plt.pie(\n",
    "        over_500_vms['VM'],\n",
    "        labels=over_500_vms.index,\n",
    "        autopct='%1.1f%%',\n",
    "        startangle=140,\n",
    "        colors=colors,\n",
    "        wedgeprops={'edgecolor': 'black', 'linewidth': 1}  # Add borders for clarity\n",
    "    )\n",
    "\n",
    "    # Improve text readability\n",
    "    for text in texts:\n",
    "        text.set_fontsize(10)  # Adjust label size\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_fontsize(10)  # Adjust percentage text size\n",
    "        autotext.set_color('white')  # Improve visibility\n",
    "\n",
    "    # Add title\n",
    "    plt.title('Operating Systems with Over 500 VMs', fontsize=12, pad=20)\n",
    "\n",
    "    # Adjust layout to prevent labels from overlapping\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0133b5-3674-4a1f-9034-6f7433e70acc",
   "metadata": {},
   "source": [
    "### 4- Group VMs by Memory Size\n",
    "\n",
    "In this section, we group the VMs by their memory-size tiers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define memory tiers (in GB) in ascending order\n",
    "memory_tiers = {\n",
    "    '0-4 GB': (0, 4),\n",
    "    '4-16 GB': (4, 16),\n",
    "    '16-32 GB': (16, 32),\n",
    "    '32-64 GB': (32, 64),\n",
    "    '64-128 GB': (64, 128),\n",
    "    '128-256 GB': (128, 256),\n",
    "    '256+ GB': (256, float('inf'))\n",
    "}\n",
    "\n",
    "# Function to categorize memory\n",
    "def categorize_memory(memory_gb):\n",
    "    for tier, (lower, upper) in memory_tiers.items():\n",
    "        if lower <= memory_gb < upper:\n",
    "            return tier\n",
    "    return 'Unknown'\n",
    "\n",
    "# Ensure 'Memory' column is in MiB, then convert to GB\n",
    "filtered_vinfo_df = filtered_vinfo_df.copy()  # Explicit copy to avoid SettingWithCopyWarning\n",
    "filtered_vinfo_df.loc[:, 'Memory GB'] = filtered_vinfo_df['Memory'] / 1024\n",
    "\n",
    "# Apply categorization\n",
    "filtered_vinfo_df.loc[:, 'Memory Tier'] = filtered_vinfo_df['Memory GB'].apply(categorize_memory)\n",
    "\n",
    "# Create a pivot table summarizing VM counts per memory tier\n",
    "memory_tier_summary = (\n",
    "    filtered_vinfo_df.groupby('Memory Tier')\n",
    "    .agg({'VM': 'count'})\n",
    "    .rename(columns={'VM': 'VM Count'})\n",
    ")\n",
    "\n",
    "# Sort the memory tiers in ascending order based on the predefined tier order\n",
    "memory_tier_summary = memory_tier_summary.reindex(memory_tiers.keys()).fillna(0)\n",
    "\n",
    "# Display the results\n",
    "try:\n",
    "    from IPython.display import display  # Works for Jupyter Notebook\n",
    "    print(\"\\nâœ… Memory Tier Summary (Sorted) âœ…\")\n",
    "    display(memory_tier_summary)\n",
    "except ImportError:\n",
    "    print(\"\\nâœ… Memory Tier Summary (Sorted) âœ…\")\n",
    "    print(memory_tier_summary)\n",
    "\n",
    "# Generate pie chart with sorted order\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(\n",
    "    memory_tier_summary['VM Count'],\n",
    "    labels=memory_tier_summary.index,\n",
    "    autopct='%1.1f%%',\n",
    "    startangle=140\n",
    ")\n",
    "plt.title(\"VM Distribution by Memory Tier (Sorted)\")\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### 5- Group VMs by Disk Size\n",
    "\n",
    "This section groups VMs into categories defined by allocated disk-size tiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e858812-f4c0-4853-8c8e-70f7542e136b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create explicit copy to avoid SettingWithCopyWarning\n",
    "filtered_vinfo_df = filtered_vinfo_df.copy()\n",
    "\n",
    "# Convert disk size to TB and categorize into tiers\n",
    "mib_to_tb_conversion_factor = 2**20 / 10**12\n",
    "filtered_vinfo_df.loc[:, 'Disk Size TB'] = filtered_vinfo_df['Provisioned MiB'] * mib_to_tb_conversion_factor\n",
    "\n",
    "# Identify VMs with missing or zero disk size\n",
    "no_disk_size_vms_df = filtered_vinfo_df[(filtered_vinfo_df['Disk Size TB'].isna()) | (filtered_vinfo_df['Disk Size TB'] == 0)]\n",
    "no_disk_size_vm_count = no_disk_size_vms_df.shape[0]\n",
    "\n",
    "# Filter out VMs without disk size for further analysis\n",
    "filtered_disk_df = filtered_vinfo_df[filtered_vinfo_df['Disk Size TB'] > 0].copy()\n",
    "\n",
    "# Define disk size bins and labels\n",
    "disk_size_bins = [0, 1, 2, 10, 20, 50, 100, float('inf')]\n",
    "disk_bin_labels = ['Tiny (<1 TB)', 'Easy (<=2 TB)', 'Medium (<=10 TB)', 'Hard (<=20 TB)', \n",
    "                   'Very Hard (<=50 TB)', 'White Glove (<=100 TB)', 'Extreme (>100 TB)']\n",
    "\n",
    "filtered_disk_df.loc[:, 'Disk Size Tiers'] = pd.cut(\n",
    "    filtered_disk_df['Disk Size TB'],\n",
    "    bins=disk_size_bins,\n",
    "    labels=disk_bin_labels\n",
    ").astype(str)\n",
    "\n",
    "# Pivot table based on disk size tiers\n",
    "disk_tier_pivot_df = filtered_disk_df.pivot_table(\n",
    "    index='Disk Size Tiers', \n",
    "    values=['VM', 'Disk Size TB'], \n",
    "    aggfunc={'VM': 'count', 'Disk Size TB': 'sum'},\n",
    "    observed=False\n",
    ")\n",
    "\n",
    "# Sort index to maintain ascending order\n",
    "disk_tier_pivot_df = disk_tier_pivot_df.reindex(disk_bin_labels)\n",
    "\n",
    "# Fill NaN values to avoid errors\n",
    "disk_tier_pivot_df = disk_tier_pivot_df.fillna(0)\n",
    "\n",
    "# Add total row with rounded 'Disk Size TB' to nearest 0.5 TB\n",
    "total_row = pd.DataFrame({\n",
    "    'VM': [disk_tier_pivot_df['VM'].sum()],\n",
    "    'Disk Size TB': [round(disk_tier_pivot_df['Disk Size TB'].sum() * 2) / 2]\n",
    "}, index=['Total'])\n",
    "\n",
    "# Combine pivot table with total row\n",
    "disk_tier_pivot_with_total = pd.concat([disk_tier_pivot_df, total_row])\n",
    "\n",
    "# Fill NaN values and format for display\n",
    "formatted_table = disk_tier_pivot_with_total.copy()\n",
    "formatted_table['VM'] = formatted_table['VM'].fillna(0).astype(int)\n",
    "formatted_table['Disk Size TB'] = formatted_table['Disk Size TB'].fillna(0).apply(lambda x: f\"{x:,.2f}\")\n",
    "formatted_table['VM'] = formatted_table['VM'].apply(lambda x: f\"{x:,}\")\n",
    "\n",
    "# Display table\n",
    "print(\"Disk Tier Summary with Total (Formatted):\")\n",
    "print(formatted_table.to_string())\n",
    "\n",
    "# Calculate percentages for VM distribution in each disk tier\n",
    "vm_counts = disk_tier_pivot_df['VM'].sum()\n",
    "percentages = (disk_tier_pivot_df['VM'] / vm_counts) * 100\n",
    "\n",
    "# Generate labels\n",
    "labels = [f\"{tier} - {pct:.1f}% ({count} VMs)\" \n",
    "          for tier, pct, count in zip(disk_tier_pivot_df.index, percentages, disk_tier_pivot_df['VM'])]\n",
    "\n",
    "# Automatically assign colors based on number of categories\n",
    "num_tiers = len(disk_bin_labels)\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, num_tiers))\n",
    "\n",
    "# Explode effect to emphasize slices\n",
    "explode = [0.1] * num_tiers  # Adjust explosion for each tier\n",
    "\n",
    "# Pie Chart: Automatically assigned colors with explode effect\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(disk_tier_pivot_df['VM'], colors=colors, autopct='%1.1f%%', startangle=140,\n",
    "        wedgeprops={'edgecolor': 'black', 'linewidth': 1}, explode=explode)\n",
    "plt.title('VM Distribution by Tier (Tiny, Easy, Medium, etc.)', fontsize=12)\n",
    "plt.legend(labels, loc=\"center left\", bbox_to_anchor=(1, 0, 0.5, 1))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Bar chart for VMs without disk size\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(['No Disk Size'], [no_disk_size_vm_count], color='purple')\n",
    "plt.title('VMs without Disk Size Information', fontsize=12)\n",
    "plt.ylabel('Number of VMs')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "### 6- Categorize Host Compute Nodes\n",
    "\n",
    "Categorize the compute nodes for ALL vCenters by their model number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Group all hosts by model\n",
    "all_host_model_pivot_df = consolidated_vhost_df.pivot_table(index=['Vendor', 'Model'], values='Host', aggfunc='count')\n",
    "display(all_host_model_pivot_df)\n",
    "\n",
    "# Bar chart for all host models\n",
    "plt.figure(figsize=(12, 8))  # Make it wider to avoid label overlap\n",
    "\n",
    "# Generate a colormap with enough distinct colors\n",
    "norm = mcolors.Normalize(vmin=0, vmax=len(all_host_model_pivot_df)-1)\n",
    "colors = cm.viridis(norm(range(len(all_host_model_pivot_df))))\n",
    "\n",
    "# Create a bar chart with automatic colors\n",
    "bar_plot = all_host_model_pivot_df['Host'].plot(kind='bar', color=colors, edgecolor='black', figsize=(12, 8))\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Host Node Compute Models', fontsize=12)\n",
    "plt.xlabel('Host Model', fontsize=10)\n",
    "plt.ylabel('Host Count', fontsize=10)\n",
    "\n",
    "# Improve x-tick readability by rotating the labels\n",
    "plt.xticks(rotation=90, ha='right', fontsize=8)\n",
    "\n",
    "# Show the bar chart\n",
    "plt.tight_layout()  # Adjust layout to prevent overlapping labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "### 7- Categorize Host Compute Nodes by vCenter\n",
    "\n",
    "Categorize compute nodes by their model number for each vCenter separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Group all the hosts by their model and vCenter\n",
    "all_host_model_vcenter_pivot_df = consolidated_vhost_df.pivot_table(\n",
    "    index=['vCenter', 'Vendor', 'Model'],\n",
    "    values=['Host'],\n",
    "    aggfunc={'Host': 'count'},\n",
    "    observed=False,\n",
    "    margins=False,\n",
    "    sort=True\n",
    ")\n",
    "\n",
    "print(f'Distribution of ALL host models by vCenter:\\n')\n",
    "print(all_host_model_vcenter_pivot_df)\n",
    "all_host_model_vcenter_pivot_df.to_clipboard(excel=True)\n",
    "\n",
    "# Bar chart creation based on the pivot table (sorted by host count)\n",
    "plt.figure(figsize=(14, 8))  # Increase figure size for better visibility\n",
    "\n",
    "# Prepare labels and data\n",
    "labels = all_host_model_vcenter_pivot_df.index.map(lambda x: f'{x[1]} {x[2]} ({x[0]})')  # Combine Vendor, Model, and vCenter in the label\n",
    "sizes = all_host_model_vcenter_pivot_df['Host']\n",
    "\n",
    "# Automatically assign colors using the 'viridis' colormap\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(sizes)))\n",
    "\n",
    "# Create the vertical bar chart\n",
    "plt.bar(labels, sizes, color=colors, edgecolor='black')\n",
    "\n",
    "# Add title and formatting\n",
    "plt.title('Host Node Compute Models', fontsize=14)\n",
    "plt.xlabel('Host Model', fontsize=12)\n",
    "plt.ylabel('Number of Hosts', fontsize=12)\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Display the bar chart\n",
    "plt.tight_layout()  # Adjust layout to prevent clipping\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "### 8- Count the ESXi Datacenters & Clusters\n",
    "\n",
    "Summary for the total ESXi datacenters & clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Remove NaN values from Datacenter and Cluster\n",
    "filtered_vinfo_df = filtered_vinfo_df.dropna(subset=['Datacenter', 'Cluster'])\n",
    "\n",
    "# Get unique Datacenters\n",
    "datacenters = filtered_vinfo_df['Datacenter'].unique()\n",
    "\n",
    "# Print total number of in-scope VMware Datacenters\n",
    "print(f'Total VMware Datacenters: {len(datacenters):,}')\n",
    "\n",
    "# Get unique Clusters\n",
    "clusters = filtered_vinfo_df['Cluster'].unique()\n",
    "\n",
    "# Print total number of in-scope ESXi clusters\n",
    "print(f'Total ESXi clusters: {len(clusters):,}')\n",
    "\n",
    "# Pivot the Cluster information on the Datacenters\n",
    "inscope_datacenter_pivot = filtered_vinfo_df.pivot_table(index='Datacenter',\n",
    "                                                          values='Cluster',\n",
    "                                                          aggfunc=pd.Series.nunique)  # Count unique clusters per datacenter\n",
    "\n",
    "# Rename column for clarity\n",
    "inscope_datacenter_pivot.rename(columns={'Cluster': 'Cluster_Count'}, inplace=True)\n",
    "\n",
    "# Sort by Cluster_Count in descending order\n",
    "inscope_datacenter_pivot = inscope_datacenter_pivot.sort_values(by='Cluster_Count', ascending=False)\n",
    "\n",
    "# Calculate the total number of clusters from the pivot table\n",
    "total_clusters_from_pivot = int(inscope_datacenter_pivot['Cluster_Count'].sum())  # Convert to Python int\n",
    "\n",
    "# Calculate the total number of datacenters\n",
    "total_datacenters = int(inscope_datacenter_pivot.shape[0])  # Convert to Python int\n",
    "\n",
    "# Show only the top 10 datacenters in the summary\n",
    "print(f'\\nDistribution of Clusters to Datacenters (Top 10 by Cluster Count):\\n')\n",
    "print(inscope_datacenter_pivot.head(10).to_string(index=True))  # Shows only top 10 sorted\n",
    "print(f'\\nTotal Clusters across all Datacenters (from pivot table): {total_clusters_from_pivot}')\n",
    "print(f'Total number of Datacenters: {total_datacenters}')\n",
    "\n",
    "# Debug: Compare Total ESXi clusters vs. Sum of clusters per datacenter\n",
    "if len(clusters) != total_clusters_from_pivot:\n",
    "    print(\"\\nâš ï¸ Warning: The total ESXi clusters count does not match the sum of clusters across datacenters.\")\n",
    "    print(f\"Total ESXi clusters (unique across dataset): {len(clusters):,}\")\n",
    "    print(f\"Total Clusters from pivot table (sum per datacenter): {total_clusters_from_pivot:,}\")\n",
    "\n",
    "    # Find clusters mapped to multiple datacenters\n",
    "    cluster_datacenter_mapping = filtered_vinfo_df.groupby('Cluster')['Datacenter'].nunique()\n",
    "    multi_dc_clusters = cluster_datacenter_mapping[cluster_datacenter_mapping > 1]\n",
    "\n",
    "    if not multi_dc_clusters.empty:\n",
    "        print(\"\\nClusters that appear in multiple datacenters (Top 10 shown):\")\n",
    "        print(multi_dc_clusters.head(10).to_string(index=True))  # Shows only top 10\n",
    "\n",
    "# Copy full results to clipboard for easy pasting into Excel\n",
    "inscope_datacenter_pivot.to_clipboard(excel=True)\n",
    "\n",
    "# Create a pie chart (using only the top 10 datacenters for clarity)\n",
    "top_10_pivot = inscope_datacenter_pivot.head(10)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(top_10_pivot['Cluster_Count'], labels=top_10_pivot.index, \n",
    "        autopct='%1.1f%%', startangle=140, explode=[0.05] * len(top_10_pivot))\n",
    "plt.title('Clusters distributed to Top 10 Datacenters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### 9- VM distribution by ESXi Clusters\n",
    "\n",
    "This is orthogonal to the VM distribution analysis by `vCenters`, as a vCenter is likely to contain multiple `clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pivot the VM information on the clusters\n",
    "inscope_cluster_pivot = inscope_vinfo_df.pivot_table(index='Cluster',\n",
    "                                                      values='VM',\n",
    "                                                      aggfunc='count')  # Count VMs per cluster\n",
    "\n",
    "# Rename column for clarity\n",
    "inscope_cluster_pivot.rename(columns={'VM': 'VM_Count'}, inplace=True)\n",
    "\n",
    "# Sort by VM_Count in descending order\n",
    "inscope_cluster_pivot = inscope_cluster_pivot.sort_values(by='VM_Count', ascending=False)\n",
    "\n",
    "# Calculate the total number of VMs\n",
    "total_vms = int(inscope_cluster_pivot['VM_Count'].sum())  # Convert to Python int\n",
    "\n",
    "# Calculate the total number of clusters\n",
    "total_clusters = int(inscope_cluster_pivot.shape[0])  # Convert to Python int\n",
    "\n",
    "# Show only the top 10 clusters in the summary\n",
    "print(f'\\nDistribution of VMs to Clusters (Top 10 by VM Count):\\n')\n",
    "print(inscope_cluster_pivot.head(10).to_string(index=True))  # Shows only top 10 sorted\n",
    "print(f'\\nTotal VMs across all clusters: {total_vms}')\n",
    "print(f'Total number of clusters: {total_clusters}')\n",
    "\n",
    "# Copy full results to clipboard for easy pasting into Excel\n",
    "inscope_cluster_pivot.to_clipboard(excel=True)\n",
    "\n",
    "# Create a pie chart (using only the top 10 clusters for clarity)\n",
    "top_10_pivot = inscope_cluster_pivot.head(10)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(top_10_pivot['VM_Count'], labels=top_10_pivot.index, \n",
    "        autopct='%1.1f%%', startangle=140, explode=[0.05] * len(top_10_pivot))\n",
    "plt.title('VMs distributed to Top 10 Clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "### 10- VMs Categorized by Environment\n",
    "\n",
    "This analysis is separate from the VM distribution by environment. The goal is to enhance the vInfo data with additional categorization.\n",
    "\n",
    "1. Create an Environment Column: This column will be based on the name of the ESXi cluster, which indicates the site location.\n",
    "2. Add a Site-Type Column: This column will categorize the environment into one of the following types: Dev, QA, Test, NonProd and Prod.\n",
    "3. Handle Unclassified Clusters: Any cluster names that do not fall into the four categories above will be grouped as \"Unknown.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### VMs Categorized by Environment\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a function to categorize 'Cluster' into environments based on 'Environment' column or 'Cluster' name\n",
    "def determine_environment(row):\n",
    "    # Ensure 'Environment' column is checked first\n",
    "    environment = str(row.get('Environment', '')).strip().lower()\n",
    "    if environment in ['nonprod', 'non-production', 'nonproduction']:\n",
    "        return 'NonProd'\n",
    "    elif environment in ['prod', 'production']:\n",
    "        return 'Prod'\n",
    "    elif environment in ['dev', 'development']:\n",
    "        return 'Dev'\n",
    "    elif environment in ['qa']:\n",
    "        return 'QA'\n",
    "    elif environment in ['test', 'UAT']:\n",
    "        return 'Test'\n",
    "\n",
    "    # If 'Environment' is empty, check 'Cluster' name for environment keywords\n",
    "    cluster_name = str(row.get('Cluster', '')).strip().lower()\n",
    "    if 'nonprod' in cluster_name:\n",
    "        return 'NonProd'\n",
    "    elif 'prod' in cluster_name:\n",
    "        return 'Prod'\n",
    "    elif 'dev' in cluster_name:\n",
    "        return 'Dev'\n",
    "    elif 'qa' in cluster_name:\n",
    "        return 'QA'\n",
    "    elif 'test' in cluster_name:\n",
    "        return 'Test'\n",
    "    \n",
    "    # Return 'Unknown' if no matching environment is found\n",
    "    return 'Unknown'\n",
    "\n",
    "# Apply the function to create or update the 'Environment' column in the DataFrame\n",
    "inscope_vinfo_df['Environment'] = inscope_vinfo_df.apply(determine_environment, axis=1)\n",
    "\n",
    "# Display the distribution of environments\n",
    "print(f\"Environment Distribution:\\n\")\n",
    "environment_counts = inscope_vinfo_df['Environment'].value_counts()\n",
    "for env, count in environment_counts.items():\n",
    "    print(f\"{env}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "### 11- VMs Graphed by Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Block Display by Environment\n",
    "\n",
    "# Count the VMs classified by each environment type\n",
    "print(f\"Total VMs in Prod   : {len(inscope_vinfo_df[inscope_vinfo_df['Environment'] == 'Prod']):,}\")\n",
    "print(f\"Total VMs in Dev    : {len(inscope_vinfo_df[inscope_vinfo_df['Environment'] == 'Dev']):,}\")\n",
    "print(f\"Total VMs in QA     : {len(inscope_vinfo_df[inscope_vinfo_df['Environment'] == 'QA']):,}\")\n",
    "print(f\"Total VMs in Test   : {len(inscope_vinfo_df[inscope_vinfo_df['Environment'] == 'Test']):,}\")\n",
    "print(f\"Total VMs in NonProd : {len(inscope_vinfo_df[inscope_vinfo_df['Environment'] == 'NonProd']):,}\")\n",
    "print(f\"Total VMs in Unknown: {len(inscope_vinfo_df[inscope_vinfo_df['Environment'] == 'Unknown']):,}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Count the number of VMs in each environment type\n",
    "env_counts = inscope_vinfo_df['Environment'].value_counts()\n",
    "\n",
    "# Plot the pie chart\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(env_counts, labels=env_counts.index, autopct='%1.1f%%', startangle=140, colors=plt.cm.tab20.colors, wedgeprops={'edgecolor': 'black'})\n",
    "plt.title('VM Distribution by Environment')\n",
    "\n",
    "# Display the pie chart\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie chart is drawn as a circle.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "### 12- Summarize Operating Systems by Supported vs. Unsupported\n",
    "\n",
    "In this section, we provide a summary of the supported and unsupported operating systems for the in-scope vCenter instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure that 'ignored_vinfo_df' represents the VMs that were filtered out\n",
    "ignored_vinfo_df = consolidated_vinfo_df[consolidated_vinfo_df['Exclusion Reason'] != '']\n",
    "\n",
    "# Function to truncate OS names for better visualization\n",
    "def truncate_os_names(series):\n",
    "    return series.rename(lambda x: x if len(x) <= 40 else x[:37] + '...')\n",
    "\n",
    "# Group and count OS occurrences for both In-Scope and Out-of-Scope VMs\n",
    "in_scope_os_counts = truncate_os_names(filtered_vinfo_df['Cleaned OS'].value_counts())\n",
    "out_of_scope_os_counts = truncate_os_names(ignored_vinfo_df['Cleaned OS'].value_counts())\n",
    "\n",
    "# Pie chart for In-Scope vs Out-of-Scope VMs\n",
    "plt.figure(figsize=(8, 8))\n",
    "scope_counts = [len(filtered_vinfo_df), len(ignored_vinfo_df)]\n",
    "labels = ['In-Scope (Filtered VMs)', 'Out-of-Scope (Ignored VMs)']\n",
    "colors = ['lightgreen', 'lightcoral']\n",
    "plt.pie(scope_counts, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140, explode=(0, 0.1))\n",
    "plt.title('Percentage of In-Scope vs Out-of-Scope VMs')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Bar chart for In-Scope OS Counts\n",
    "plt.figure(figsize=(14, 6))\n",
    "in_scope_os_counts.sort_values(ascending=False).plot(kind='bar', color='green')\n",
    "plt.title('In-Scope OS Counts')\n",
    "plt.xlabel('Operating System')\n",
    "plt.ylabel('Number of VMs')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Bar chart for Out-of-Scope OS Counts\n",
    "plt.figure(figsize=(14, 6))\n",
    "out_of_scope_os_counts.sort_values(ascending=False).plot(kind='bar', color='red')\n",
    "plt.title('Out-of-Scope OS Counts')\n",
    "plt.xlabel('Operating System')\n",
    "plt.ylabel('Number of VMs')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "### 13- Migration Complexity by OS\n",
    "\n",
    "Categorized OSâ€™s into\n",
    "\n",
    "* Easy\n",
    "* Medium\n",
    "* Hard\n",
    "* Database\n",
    "* Unsupported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Categorize supported OS into difficulty levels\n",
    "os_difficulty_mapping = {\n",
    "    'Windows': 'Medium',\n",
    "    'Red Hat': 'Easy',\n",
    "    'Ubuntu': 'Hard',\n",
    "    'Suse': 'Hard',\n",
    "    'Oracle': 'Database',\n",
    "    'Microsoft SQL': 'Database'\n",
    "}\n",
    "\n",
    "# Ensure we only count OS instances from in-scope VMs\n",
    "in_scope_os_counts = filtered_vinfo_df['Cleaned OS'].value_counts()\n",
    "\n",
    "# Initialize difficulty counts\n",
    "difficulty_counts = {'Easy': 0, 'Medium': 0, 'Hard': 0, 'Database': 0, 'Unsupported': 0}\n",
    "\n",
    "# Map OS instances to difficulty levels\n",
    "for os_name, count in in_scope_os_counts.items():\n",
    "    difficulty = next((difficulty for key, difficulty in os_difficulty_mapping.items() if key in os_name), 'Unsupported')\n",
    "    difficulty_counts[difficulty] += count\n",
    "\n",
    "# Identify unsupported OS instances within in-scope VMs\n",
    "unsupported_count = difficulty_counts['Unsupported']\n",
    "\n",
    "# Print White Glove OS instances\n",
    "white_glove_os = [os for os in in_scope_os_counts.index if 'Oracle' in os or 'Microsoft SQL' in os]\n",
    "print(\"\\n\\U0001F4DD White Glove OS Instances:\")\n",
    "for os in white_glove_os:\n",
    "    print(f\"\\U0001F539 {os}: {in_scope_os_counts[os]}\")\n",
    "\n",
    "# Create a bar chart with automatic colors\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "difficulty_levels = list(difficulty_counts.keys())\n",
    "os_counts = list(difficulty_counts.values())\n",
    "\n",
    "# Generate a colormap automatically\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(difficulty_levels)))\n",
    "\n",
    "bars = plt.bar(difficulty_levels, os_counts, color=colors)\n",
    "plt.title('Migration Complexity by OS')\n",
    "plt.xlabel('Difficulty Level')\n",
    "plt.ylabel('Number of OS Instances')\n",
    "\n",
    "# Add numeric labels on bars\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.5, yval, ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "### 14- Migration Complexity by Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f82ddb2-4864-43d6-9f0b-13f4b4e368a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define disk size categories\n",
    "size_bins = [0, 2, 10, 20, 50, float('inf')]\n",
    "size_labels = ['Tiny (<=2TB)', 'Easy (2-10TB)', 'Medium (10-20TB)', 'Hard (20-50TB)', 'White Glove (>50TB)']\n",
    "\n",
    "# Categorize VMs by disk size\n",
    "filtered_vinfo_df['Disk Size Category'] = pd.cut(filtered_vinfo_df['Disk Size TB'], bins=size_bins, labels=size_labels)\n",
    "\n",
    "# Create summary\n",
    "disk_size_summary = filtered_vinfo_df['Disk Size Category'].value_counts().reindex(size_labels).reset_index()\n",
    "disk_size_summary.columns = ['Disk Size Category', 'VM Count']\n",
    "\n",
    "# Display summary\n",
    "print(\"\\nðŸ“Š VM Totals by Disk Size Category:\")\n",
    "print(disk_size_summary.to_string(index=False))\n",
    "\n",
    "# Generate dynamic colors\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(size_labels)))\n",
    "\n",
    "# Plot bar chart with dynamic colors\n",
    "bars = plt.bar(disk_size_summary['Disk Size Category'], disk_size_summary['VM Count'], color=colors)\n",
    "\n",
    "plt.xlabel('Disk Size Category')\n",
    "plt.ylabel('Number of VMs')\n",
    "plt.title('VM Totals by Disk Size Category')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Add numeric labels on bars\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.5, yval, ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b541e50e-6642-4a7a-a449-6b3dee2f2f69",
   "metadata": {},
   "source": [
    "### 15- Migration Complexity by Disk & OS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create explicit copy to avoid SettingWithCopyWarning\n",
    "filtered_vinfo_df = filtered_vinfo_df.copy()\n",
    "\n",
    "# Define disk size categories\n",
    "size_bins = [0, 2, 10, 20, 50, float('inf')]\n",
    "size_labels = ['Tiny (<=2TB)', 'Easy (2-10TB)', 'Medium (10-20TB)', 'Hard (20-50TB)', 'White Glove (>50TB)']\n",
    "\n",
    "filtered_vinfo_df['Disk Size Category'] = pd.cut(filtered_vinfo_df['Disk Size TB'], bins=size_bins, labels=size_labels)\n",
    "\n",
    "# OS difficulty mapping\n",
    "os_difficulty_mapping = {\n",
    "    'Windows': 'Medium',\n",
    "    'Red Hat': 'Easy',\n",
    "    'Ubuntu': 'Hard',\n",
    "    'Suse': 'Hard',\n",
    "    'Oracle': 'Database',\n",
    "    'Microsoft SQL': 'Database'\n",
    "}\n",
    "\n",
    "# Map OS to difficulty level\n",
    "filtered_vinfo_df['OS Difficulty'] = filtered_vinfo_df['Cleaned OS'].apply(\n",
    "    lambda os: next((difficulty for key, difficulty in os_difficulty_mapping.items() if key in os), 'Unsupported')\n",
    ")\n",
    "\n",
    "# Create a pivot table summarizing VMs by Disk Size and OS Difficulty\n",
    "pivot_summary = filtered_vinfo_df.pivot_table(\n",
    "    index='Disk Size Category',\n",
    "    columns='OS Difficulty',\n",
    "    values='VM',\n",
    "    aggfunc='count',\n",
    "    fill_value=0,\n",
    "    observed=True\n",
    ").reset_index()\n",
    "\n",
    "# Display pivot summary\n",
    "print(\"\\nðŸ“Š Migration Complexity by Disk Size Category and OS Difficulty:\")\n",
    "print(pivot_summary.to_string(index=False, header=True))\n",
    "\n",
    "# Generate dynamic colors based on the number of OS difficulty categories\n",
    "num_categories = len(pivot_summary.columns) - 1  # Excluding 'Disk Size Category'\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, num_categories))\n",
    "\n",
    "# Plot stacked bar chart with dynamic colors\n",
    "ax = pivot_summary.set_index('Disk Size Category').plot(\n",
    "    kind='bar',\n",
    "    stacked=True,\n",
    "    figsize=(12, 7),\n",
    "    colormap=plt.cm.viridis  # Apply dynamic colormap\n",
    ")\n",
    "\n",
    "plt.xlabel('Disk Size Category')\n",
    "plt.ylabel('Number of VMs')\n",
    "plt.title('\\nMigration Complexity by Disk Size Category and OS Difficulty Level')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title='OS Difficulty', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "### 16- Estimated Migration Time by OS\n",
    "\n",
    "FTE_COUNT can be changed to match the number of engineers that will be delivering the engagement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Constants\n",
    "migration_time_per_500gb = 110  # minutes (1 hour 50 minutes per 500GB)\n",
    "te_hours_per_day = 8            # 8 hours per day per FTE\n",
    "fte_count = 10                  # 5 FTEs available\n",
    "pmt_hours = 1                   # Post-migration troubleshooting time per VM in hours\n",
    "\n",
    "# Ensure required columns are present in filtered_vinfo_df\n",
    "required_columns = {'Cleaned OS', 'Disk Size TB', 'VM', 'Cluster'}\n",
    "available_columns = set(filtered_vinfo_df.columns)\n",
    "optional_columns = {'Environment'}  # Optional columns\n",
    "missing_columns = required_columns - available_columns\n",
    "\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"The following required columns are missing in filtered_vinfo_df: {missing_columns}\")\n",
    "\n",
    "# Assign filtered_vinfo_df to inscope_vinfo_df to maintain consistency\n",
    "inscope_vinfo_df = filtered_vinfo_df.copy()\n",
    "\n",
    "# Preprocess columns\n",
    "inscope_vinfo_df['Cleaned OS'] = inscope_vinfo_df['Cleaned OS'].fillna(\"Unknown OS\").astype(str).str.strip()\n",
    "inscope_vinfo_df['Disk Size TB'] = pd.to_numeric(inscope_vinfo_df['Disk Size TB'], errors='coerce').fillna(0)\n",
    "inscope_vinfo_df['VM'] = inscope_vinfo_df['VM'].fillna('Unknown VM')\n",
    "inscope_vinfo_df['Cluster'] = inscope_vinfo_df['Cluster'].fillna('').str.lower()\n",
    "\n",
    "if 'Environment' in inscope_vinfo_df.columns:\n",
    "    inscope_vinfo_df['Environment'] = inscope_vinfo_df['Environment'].fillna('unknown').str.lower().str.strip()\n",
    "\n",
    "# Define disk size classification bins\n",
    "size_bins = [0, 2, 10, 20, 50, float('inf')]\n",
    "size_labels = ['Tiny (<=2TB)', 'Easy (2-10TB)', 'Medium (10-20TB)', 'Hard (20-50TB)', 'White Glove (>50TB)']\n",
    "\n",
    "# Assign Disk Size Category\n",
    "inscope_vinfo_df['Disk Size Category'] = pd.cut(inscope_vinfo_df['Disk Size TB'], bins=size_bins, labels=size_labels)\n",
    "\n",
    "# Generate supported OS list dynamically\n",
    "def truncate_os_names(os_series):\n",
    "    \"\"\"Extract unique OS names for classification.\"\"\"\n",
    "    return os_series.index.tolist()\n",
    "\n",
    "supported_os_counts = truncate_os_names(filtered_vinfo_df['Cleaned OS'].value_counts())\n",
    "\n",
    "# Assign complexities based on classification\n",
    "def classify_complexity(row):\n",
    "    if 'sql-' in row['Cluster']:\n",
    "        return 'MSSQL-DBs'\n",
    "    elif 'oracle' in row['Cluster']:\n",
    "        return 'Oracle-DBs'\n",
    "    elif row['Disk Size Category'] == 'Tiny (<=2TB)':\n",
    "        return 'Tiny'\n",
    "    elif row['Disk Size Category'] == 'Easy (2-10TB)':\n",
    "        return 'Easy'\n",
    "    elif row['Disk Size Category'] == 'Medium (10-20TB)':\n",
    "        return 'Medium'\n",
    "    elif row['Disk Size Category'] == 'Hard (20-50TB)':\n",
    "        return 'Hard'\n",
    "    elif row['Disk Size Category'] == 'White Glove (>50TB)':\n",
    "        return 'White Glove'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "inscope_vinfo_df['Complexity'] = inscope_vinfo_df.apply(classify_complexity, axis=1)\n",
    "\n",
    "# Define complexity sorting order\n",
    "complexity_order = ['Tiny', 'Easy', 'Medium', 'Hard', 'White Glove', 'Oracle-DBs', 'MSSQL-DBs']\n",
    "inscope_vinfo_df['Complexity'] = pd.Categorical(\n",
    "    inscope_vinfo_df['Complexity'],\n",
    "    categories=complexity_order,\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "# Sort DataFrame\n",
    "inscope_vinfo_df = inscope_vinfo_df.sort_values('Complexity')\n",
    "\n",
    "# Assign OS Support dynamically\n",
    "inscope_vinfo_df['OS Support'] = inscope_vinfo_df['Cleaned OS'].apply(\n",
    "    lambda os: 'Supported' if any(supported_os in os for supported_os in supported_os_counts) else 'Not Supported'\n",
    ")\n",
    "\n",
    "# Compute Migration Time\n",
    "inscope_vinfo_df['Migration Time (minutes)'] = inscope_vinfo_df['Disk Size TB'].apply(\n",
    "    lambda size: ((size * 1024) / 500) * migration_time_per_500gb\n",
    ")\n",
    "\n",
    "# Compute Total Time (Migration + Post-Migration)\n",
    "pmt_minutes = pmt_hours * 60\n",
    "inscope_vinfo_df['Total Time (minutes)'] = inscope_vinfo_df['Migration Time (minutes)'] + pmt_minutes\n",
    "\n",
    "# Summarize data by Complexity and OS Support\n",
    "disk_classification_summary = inscope_vinfo_df.groupby(['Complexity', 'OS Support'], observed=True).agg(\n",
    "    VM_Count=('VM', 'count'),\n",
    "    Total_Disk=('Disk Size TB', 'sum'),\n",
    "    Total_Mig_Time_Minutes=('Total Time (minutes)', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Ensure all Complexity and OS Support combinations exist\n",
    "for complexity in complexity_order:\n",
    "    for os_support in ['Supported', 'Not Supported']:\n",
    "        if not ((disk_classification_summary['Complexity'] == complexity) &\n",
    "                (disk_classification_summary['OS Support'] == os_support)).any():\n",
    "            disk_classification_summary = pd.concat([\n",
    "                disk_classification_summary,\n",
    "                pd.DataFrame({\n",
    "                    'Complexity': [complexity],\n",
    "                    'OS Support': [os_support],\n",
    "                    'VM_Count': [0],\n",
    "                    'Total_Disk': [0],\n",
    "                    'Total_Mig_Time_Minutes': [0]\n",
    "                })\n",
    "            ], ignore_index=True)\n",
    "\n",
    "# Remove rows where VM_Count is 0\n",
    "disk_classification_summary = disk_classification_summary[disk_classification_summary['VM_Count'] > 0]\n",
    "\n",
    "disk_classification_summary['Formatted_Mig_Time'] = disk_classification_summary['Total_Mig_Time_Minutes'].apply(\n",
    "    lambda minutes: f\"{minutes / 60:,.1f}h\"\n",
    ")\n",
    "\n",
    "disk_classification_summary['Total_Days'] = disk_classification_summary['Total_Mig_Time_Minutes'].apply(\n",
    "    lambda minutes: f\"{minutes / (te_hours_per_day * 60 * fte_count):,.1f}\"\n",
    ")\n",
    "\n",
    "def format_disk_size(tb_value):\n",
    "    return f\"{tb_value * 1024:,.0f} GB\" if tb_value < 1 else f\"{tb_value:,.2f} TB\"\n",
    "\n",
    "total_disk_tb_numeric = disk_classification_summary['Total_Disk'].astype(float).sum()\n",
    "disk_classification_summary['Total_Disk'] = disk_classification_summary['Total_Disk'].astype(float).apply(format_disk_size)\n",
    "\n",
    "total_mig_time_minutes = disk_classification_summary['Total_Mig_Time_Minutes'].sum()\n",
    "\n",
    "totals_row = {\n",
    "    'Complexity': 'Totals',\n",
    "    'OS Support': '',\n",
    "    'VM_Count': f\"{disk_classification_summary['VM_Count'].astype(str).replace(',', '', regex=True).astype(int).sum():,}\",\n",
    "    'Total_Disk': format_disk_size(total_disk_tb_numeric),\n",
    "    'Formatted_Mig_Time': f\"{total_mig_time_minutes / 60:,.1f}h\",\n",
    "    'Total_Days': f\"{total_mig_time_minutes / (te_hours_per_day * 60 * fte_count):,.1f}\",\n",
    "    'Total_Mig_Time_Minutes': total_mig_time_minutes\n",
    "}\n",
    "disk_classification_summary = pd.concat([disk_classification_summary, pd.DataFrame([totals_row])], ignore_index=True)\n",
    "\n",
    "disk_classification_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3a5090-41a4-48c2-8f84-2afec0cf6089",
   "metadata": {},
   "source": [
    "### 17- Estimated Migration Time by vCenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9334fa83-e0f0-4c23-b4df-af8f97541f69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Constants\n",
    "fte_hours_per_day = 8  # 8 hours per day per FTE\n",
    "fte_count = 10         # 10 FTEs available\n",
    "\n",
    "complexity_order = ['Tiny', 'Easy', 'Medium', 'Hard', 'White Glove', 'Oracle-DBs', 'MSSQL-DBs']\n",
    "inscope_vinfo_df['Complexity'] = pd.Categorical(\n",
    "    inscope_vinfo_df['Complexity'],\n",
    "    categories=complexity_order,\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "# Ensure the vCenter column exists\n",
    "if 'vCenter' not in inscope_vinfo_df.columns:\n",
    "    raise ValueError(\"The required column 'vCenter' is missing from inscope_vinfo_df.\")\n",
    "\n",
    "# Get unique vCenters\n",
    "vcenters = inscope_vinfo_df['vCenter'].unique()\n",
    "\n",
    "# Dictionary to store summaries for each vCenter\n",
    "vcenter_summaries = {}\n",
    "\n",
    "# Function to format the table output\n",
    "def custom_table_format_with_totals(headers, rows):\n",
    "    horizontal_line = \"â”€\"\n",
    "    vertical_line = \"â”‚\"\n",
    "    corner_tl, corner_tr = \"â•­\", \"â•®\"\n",
    "    corner_bl, corner_br = \"â•°\", \"â•¯\"\n",
    "    join_t, join_b, join_c = \"â”¬\", \"â”´\", \"â”¼\"\n",
    "    col_widths = [max(len(str(item)) for item in col) for col in zip(headers, *rows)]\n",
    "    \n",
    "    def make_row(items):\n",
    "        return vertical_line + vertical_line.join(f\"{str(item).rjust(width)}\" for item, width in zip(items, col_widths)) + vertical_line\n",
    "    \n",
    "    top_line = corner_tl + join_t.join(horizontal_line * width for width in col_widths) + corner_tr\n",
    "    header_row = make_row(headers)\n",
    "    divider_row = join_c.join(horizontal_line * width for width in col_widths).join([\"â”œ\", \"â”¤\"])\n",
    "    data_rows = [make_row(row) for row in rows[:-1]]\n",
    "    totals_divider_row = join_c.join(horizontal_line * width for width in col_widths).join([\"â”œ\", \"â”¤\"])\n",
    "    totals_row = make_row(rows[-1])\n",
    "    bottom_line = corner_bl + join_b.join(horizontal_line * width for width in col_widths) + corner_br\n",
    "    return \"\\n\".join([top_line, header_row, divider_row] + data_rows + [totals_divider_row, totals_row, bottom_line])\n",
    "\n",
    "# Process each vCenter separately\n",
    "for vcenter in vcenters:\n",
    "    # Filter the DataFrame for the current vCenter\n",
    "    vcenter_df = inscope_vinfo_df[inscope_vinfo_df['vCenter'] == vcenter]\n",
    "\n",
    "    # Perform calculations on the filtered DataFrame\n",
    "    disk_classification_summary = vcenter_df.groupby(['Complexity', 'OS Support'], observed=True).agg(\n",
    "        VM_Count=('VM', 'count'),\n",
    "        Total_Disk=('Disk Size TB', 'sum'),\n",
    "        Total_Mig_Time_Minutes=('Total Time (minutes)', 'sum')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Ensure all \"Complexity\" and \"OS Support\" categories exist\n",
    "    for complexity in complexity_order:\n",
    "        for os_support in ['Supported', 'Not Supported']:\n",
    "            if not ((disk_classification_summary['Complexity'] == complexity) &\n",
    "                    (disk_classification_summary['OS Support'] == os_support)).any():\n",
    "                disk_classification_summary = pd.concat([\n",
    "                    disk_classification_summary,\n",
    "                    pd.DataFrame({\n",
    "                        'Complexity': [complexity],\n",
    "                        'OS Support': [os_support],\n",
    "                        'VM_Count': [0],\n",
    "                        'Total_Disk': [0.0],\n",
    "                        'Total_Mig_Time_Minutes': [0]\n",
    "                    })\n",
    "                ], ignore_index=True)\n",
    "\n",
    "    # Remove rows where VM_Count is 0\n",
    "    disk_classification_summary = disk_classification_summary[disk_classification_summary['VM_Count'] > 0]\n",
    "\n",
    "    # Convert 'Complexity' to a categorical type with the custom order\n",
    "    disk_classification_summary['Complexity'] = pd.Categorical(\n",
    "        disk_classification_summary['Complexity'],\n",
    "        categories=complexity_order,\n",
    "        ordered=True\n",
    "    )\n",
    "\n",
    "    # Sort the summary by the custom complexity order\n",
    "    disk_classification_summary = disk_classification_summary.sort_values('Complexity')\n",
    "\n",
    "    # Store numeric values separately before formatting\n",
    "    total_disk_tb_numeric = disk_classification_summary['Total_Disk'].astype(float).sum()\n",
    "    total_mig_time_minutes = disk_classification_summary['Total_Mig_Time_Minutes'].sum()\n",
    "\n",
    "    # Add calculated columns for formatted migration time and days\n",
    "    disk_classification_summary['Formatted_Mig_Time'] = disk_classification_summary['Total_Mig_Time_Minutes'].apply(\n",
    "        lambda minutes: f\"{minutes / 60:,.1f}h\"\n",
    "    )\n",
    "    disk_classification_summary['Days_Per_FTEs'] = disk_classification_summary['Total_Mig_Time_Minutes'].apply(\n",
    "        lambda minutes: f\"{minutes / (fte_hours_per_day * 60 * fte_count):,.1f}\"\n",
    "    )\n",
    "\n",
    "    # Format numerical values correctly\n",
    "    disk_classification_summary['VM_Count'] = disk_classification_summary['VM_Count'].astype(int).apply(lambda x: f\"{x:,}\")\n",
    "    disk_classification_summary['Total_Disk'] = disk_classification_summary['Total_Disk'].astype(float).apply(lambda x: f\"{x:,.0f}\")\n",
    "\n",
    "    # Compute and append totals row\n",
    "    totals_row = {\n",
    "        'Complexity': 'Totals',\n",
    "        'OS Support': '',\n",
    "        'VM_Count': f\"{disk_classification_summary['VM_Count'].astype(str).replace(',', '', regex=True).astype(int).sum():,}\",\n",
    "        'Total_Disk': f\"{total_disk_tb_numeric:,.0f}\",\n",
    "        'Formatted_Mig_Time': f\"{total_mig_time_minutes / 60:,.1f}h\",\n",
    "        'Days_Per_FTEs': f\"{total_mig_time_minutes / (fte_hours_per_day * 60 * fte_count):,.1f}\"\n",
    "    }\n",
    "    disk_classification_summary = pd.concat(\n",
    "        [disk_classification_summary, pd.DataFrame([totals_row])],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # Store the summary in the dictionary\n",
    "    vcenter_summaries[vcenter] = disk_classification_summary\n",
    "\n",
    "    # Print the table for the current vCenter\n",
    "    print(f\"\\nSummary Table for vCenter: {vcenter}\")\n",
    "    headers = [\n",
    "        \"Complexity\", \"OS Support\", \"VM Count\", \"Total Disk (TB)\",\n",
    "        \"Total Migration Time\", f\"Total Days\"\n",
    "    ]\n",
    "    rows = disk_classification_summary[\n",
    "        ['Complexity', 'OS Support', 'VM_Count', 'Total_Disk', 'Formatted_Mig_Time', 'Days_Per_FTEs']\n",
    "    ].values.tolist()\n",
    "    print(custom_table_format_with_totals(headers, rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6977fd21-5e69-4260-9664-1d0d99b9ebb9",
   "metadata": {},
   "source": [
    "### 18- Estimated Migration Time (Summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd9bb6a-587e-4c7b-898d-098d91cbea79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate global totals across all vCenters\n",
    "global_total_mig_time = sum(\n",
    "    summary['Total_Mig_Time_Minutes'].sum() for summary in vcenter_summaries.values()\n",
    ")\n",
    "global_total_days = global_total_mig_time / (fte_hours_per_day * 60 * fte_count)\n",
    "\n",
    "# Ensure all values are correctly formatted\n",
    "print(f\"\\nGlobal Total Migration Time: {global_total_mig_time / 60:,.1f}h\")\n",
    "print(f\"Global Total Days: {global_total_days:,.1f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
